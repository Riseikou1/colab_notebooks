{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNa9+S46USftciu4i10eO7+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# BackProp\n","\n","this time, we will be manually writing all the fucking backpropagation shits"],"metadata":{"id":"HzNTl-zUHO-w"}},{"cell_type":"markdown","source":["## Starter Code\n","\n","almost identical to our part-3 code"],"metadata":{"id":"sRUzf7auHPBS"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"ab8oATnmF9nN","executionInfo":{"status":"ok","timestamp":1746782935865,"user_tz":-540,"elapsed":156,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"36f0a99d-f5c5-4b2e-de0d-547de9ba9915"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-05-09 09:28:55--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 228145 (223K) [text/plain]\n","Saving to: ‘names.txt.5’\n","\n","\rnames.txt.5           0%[                    ]       0  --.-KB/s               \rnames.txt.5         100%[===================>] 222.80K  --.-KB/s    in 0.02s   \n","\n","2025-05-09 09:28:55 (10.9 MB/s) - ‘names.txt.5’ saved [228145/228145]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n"]},{"cell_type":"code","source":["words = open(\"names.txt\",\"r\").read().splitlines()"],"metadata":{"id":"WuSXNmQ-HPKX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chars = sorted(list(set(\"\".join(words))))\n","stoi = {s : i+1 for i,s in enumerate(chars)}\n","stoi[\".\"] = 0\n","itos = {i:s for s,i in stoi.items()}\n","block_size = 3\n","vocab_size = len(itos)"],"metadata":{"id":"KIq-1JqcHPND"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import torch.nn as nn"],"metadata":{"id":"efi6bEiEHPPl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_dataset(words):\n","    X,Y = [],[]\n","    for w in words :\n","        context = [0] * block_size\n","        for ch in w + \".\":\n","            ix = stoi[ch]\n","            Y.append(ix)\n","            X.append(context)\n","            context = context[1:] + [ix]\n","\n","    X = torch.tensor(X)\n","    Y = torch.tensor(Y)\n","    print(X.shape, Y.shape)\n","    return X,Y\n","\n","import random\n","random.seed(42)\n","random.shuffle(words)\n","n1 = int(0.8 * len(words))\n","n2 = int(0.9*len(words))\n","\n","Xtr,Ytr = build_dataset(words[:n1])\n","Xdev,Ydev = build_dataset(words[n1:n2])\n","Xte,Yte = build_dataset(words[n2:])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HyU3rkTkHPR9","executionInfo":{"status":"ok","timestamp":1746782936622,"user_tz":-540,"elapsed":758,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"82826aed-cc19-46be-f1c2-5ae56efd83fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([182625, 3]) torch.Size([182625])\n","torch.Size([22655, 3]) torch.Size([22655])\n","torch.Size([22866, 3]) torch.Size([22866])\n"]}]},{"cell_type":"code","source":["n_embed = 10\n","n_hidden = 200\n","\n","g= torch.Generator().manual_seed(2147483647)\n","C = torch.randn((vocab_size,n_embed),generator=g)\n","W1 = torch.randn((block_size * n_embed , n_hidden),generator=g) * (5/3) / ((n_embed*block_size) ** 0.5)\n","b1 = torch.randn(n_hidden,generator=g)  * 0.01\n","W2 = torch.randn((n_hidden,vocab_size), generator=g)  * 0.01\n","b2 = torch.randn(vocab_size,generator=g) * 0\n","\n","bngain = torch.ones((1,n_hidden))\n","bnbias = torch.zeros((1,n_hidden))\n","bnmean_running = torch.zeros((1,n_hidden))\n","bnstd_running = torch.ones((1,n_hidden))\n","\n","parameters = [C,W1,b1,W2,b2,bngain,bnbias]\n","print(sum(p.nelement() for p in parameters))\n","for p in parameters :\n","    p.requires_grad = True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-4w10q1pHPUa","executionInfo":{"status":"ok","timestamp":1746782936632,"user_tz":-540,"elapsed":9,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"b3070510-1dbc-4e6b-f322-f48f6acb36f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12297\n"]}]},{"cell_type":"code","source":["max_steps = 200000\n","batch_size = 32\n","lossi = []\n","\n","for i in range(max_steps):\n","    ix = torch.randint(0,Xtr.shape[0],(batch_size,),generator=g)\n","    Xb,Yb = Xtr[ix], Ytr[ix]\n","\n","    # forward pass\n","    emb = C[Xb]\n","    embcat = emb.view(emb.shape[0],-1)\n","    hpreact = embcat @ W1 + b1\n","    bnmeani = hpreact.mean(0,keepdims=True)\n","    bnstdi = hpreact.std(0,keepdims=True)\n","\n","    hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n","\n","    with torch.no_grad():\n","        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n","        bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n","\n","    h = torch.tanh(hpreact)\n","    logits = h @ W2 + b2\n","    loss = F.cross_entropy(logits,Yb)\n","\n","    # backward pass\n","    for p in parameters :\n","        p.grad = None\n","    loss.backward()\n","\n","    lr = 0.1 if i < 100000 else 0.01\n","    for p in parameters :\n","        p.data += -lr * p.grad\n","\n","    if i % 10000 == 0:\n","        print(f\"{i:7d}/{max_steps:7d} : {loss.item():.4f}\")\n","    lossi.append(loss.log10().item())\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LodAhuCAHPW5","executionInfo":{"status":"ok","timestamp":1746782936641,"user_tz":-540,"elapsed":10,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"9af22bdc-fef7-4372-efa7-aad9e3f80c27"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["      0/ 200000 : 3.3147\n"]}]},{"cell_type":"markdown","source":["## Now it is time for becoming backprop NINJA !!!"],"metadata":{"id":"0Ui-ZenWN2O0"}},{"cell_type":"markdown","source":["utility function for comparing manual gradients to PyTorch's gradients"],"metadata":{"id":"ndfFb9tuOEdI"}},{"cell_type":"code","source":["# utility function we will use later when comparing manual gradients to PyTorch gradients\n","\n","def cmp(s,dt,t):\n","    ex = torch.all(dt == t.grad).item()\n","    app = torch.allclose(dt,t.grad)\n","    maxdiff = (dt - t.grad).abs().max().item()\n","    print(f\"{s:15s} | exact : {str(ex):5s} | approximate : {str(app):5s} | maxdiff : {maxdiff}\")"],"metadata":{"id":"Byq8cAtrN1Bs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Again shits"],"metadata":{"id":"gtES7yHUO43d"}},{"cell_type":"markdown","source":["in here initializaions are little bit different. we're not making anything 0"],"metadata":{"id":"9uH5uR8cOthW"}},{"cell_type":"code","source":["n_embed = 10\n","n_hidden = 200\n","\n","g= torch.Generator().manual_seed(2147483647)\n","C = torch.randn((vocab_size,n_embed),generator=g)\n","# Layer 1\n","W1 = torch.randn((block_size * n_embed , n_hidden),generator=g) * (5/3) / ((n_embed*block_size) ** 0.5)\n","b1 = torch.randn(n_hidden,generator=g)  * 0.1\n","# Layer 2\n","W2 = torch.randn((n_hidden,vocab_size), generator=g)  * 0.1\n","b2 = torch.randn(vocab_size,generator=g) * 0.1\n","\n","# BatchNorm parameters --> scaling and shifting.\n","bngain = torch.randn((1,n_hidden)) * 0.1 + 1.0\n","bnbias = torch.randn((1,n_hidden)) * 0.1\n","\n","parameters = [C,W1,b1,W2,b2,bngain,bnbias]\n","print(sum(p.nelement() for p in parameters))\n","for p in parameters :\n","    p.requires_grad = True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZ5A5SiTHPZS","executionInfo":{"status":"ok","timestamp":1746801097294,"user_tz":-540,"elapsed":53,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"8b9c7792-b741-4478-d3a6-37bdc1d294b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12297\n"]}]},{"cell_type":"code","source":["max_steps = 200000\n","batch_size = 32\n","n = batch_size\n","ix = torch.randint(0,Xtr.shape[0],(batch_size,),generator=g)\n","Xb,Yb = Xtr[ix], Ytr[ix]   # batch X,Y"],"metadata":{"id":"KwGxnCztPeyV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n","\n","emb = C[Xb] # embed the characters into vectors\n","embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n","# Linear layer 1\n","hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n","# BatchNorm layer\n","bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n","bndiff = hprebn - bnmeani\n","bndiff2 = bndiff**2\n","bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n","bnvar_inv = (bnvar + 1e-5)**-0.5\n","bnraw = bndiff * bnvar_inv\n","hpreact = bngain * bnraw + bnbias\n","# Non-linearity\n","h = torch.tanh(hpreact) # hidden layer\n","# Linear layer 2\n","logits = h @ W2 + b2 # output layer\n","# cross entropy loss (same as F.cross_entropy(logits, Yb))\n","logit_maxes = logits.max(1, keepdim=True).values\n","norm_logits = logits - logit_maxes # subtract max for numerical stability\n","counts = norm_logits.exp()\n","counts_sum = counts.sum(1, keepdims=True)\n","counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n","probs = counts * counts_sum_inv\n","logprobs = probs.log()\n","loss = -logprobs[range(n), Yb].mean()\n","\n","# PyTorch backward pass\n","for p in parameters:\n","  p.grad = None\n","for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n","          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n","         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n","         embcat, emb]:\n","  t.retain_grad()\n","loss.backward()\n","loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WyK7jBliHPbU","executionInfo":{"status":"ok","timestamp":1746801233065,"user_tz":-540,"elapsed":58,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"7fe29274-74ed-44f4-ac81-0673f2ed4286"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(3.7803, grad_fn=<NegBackward0>)"]},"metadata":{},"execution_count":301}]},{"cell_type":"markdown","source":["## Exercise - 1"],"metadata":{"id":"a1iM7nD_pcD9"}},{"cell_type":"code","source":["# Exercise 1: backprop through the whole thing manually,\n","# backpropagating through exactly all of the variables\n","# as they are defined in the forward pass above, one by one\n","\n","dlogprobs = torch.zeros_like(logprobs)\n","dlogprobs[range(n), Yb] = -1.0/n\n","dprobs = (1.0 / probs) * dlogprobs\n","dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n","dcounts = counts_sum_inv * dprobs\n","dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n","dcounts += torch.ones_like(counts) * dcounts_sum\n","dnorm_logits = counts * dcounts\n","dlogits = dnorm_logits.clone()\n","dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n","dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n","dh = dlogits @ W2.T\n","dW2 = h.T @ dlogits\n","db2 = dlogits.sum(0)\n","dhpreact = (1.0 - h**2) * dh\n","dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n","dbnraw = bngain * dhpreact\n","dbnbias = dhpreact.sum(0, keepdim=True)\n","dbndiff = bnvar_inv * dbnraw\n","dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n","dbnvar = (-0.5 * (bnvar +1e-5)** -1.5) * dbnvar_inv\n","dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n","dbndiff += 2* bndiff * dbndiff2\n","dbnmeani = (-1 * dbndiff).sum(0,keepdims=True)\n","dhprebn  = dbndiff + ((1/n)*torch.ones_like(hprebn)) * dbnmeani\n","dembcat = dhprebn @ W1.T\n","dW1 = embcat.T @ dhprebn\n","db1 = dhprebn.sum(0,keepdims=True)\n","demb = dembcat.view(emb.shape)\n","\n","dC = torch.zeros_like(C)\n","dC.index_add_(0, Xb.view(-1), demb.view(-1, 10))\n","\n","cmp('logprobs', dlogprobs, logprobs)\n","cmp('probs', dprobs, probs)\n","cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n","cmp('counts_sum', dcounts_sum, counts_sum)\n","cmp('counts', dcounts, counts)\n","cmp('norm_logits', dnorm_logits, norm_logits)\n","cmp('logit_maxes', dlogit_maxes, logit_maxes)\n","cmp('logits', dlogits, logits)\n","cmp('h', dh, h)\n","cmp('W2', dW2, W2)\n","cmp('b2', db2, b2)\n","cmp('hpreact', dhpreact, hpreact)\n","cmp('bngain', dbngain, bngain)\n","cmp('bnbias', dbnbias, bnbias)\n","cmp('bnraw', dbnraw, bnraw)\n","cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n","cmp('bnvar', dbnvar, bnvar)\n","cmp('bndiff2', dbndiff2, bndiff2)\n","cmp('bndiff', dbndiff, bndiff)\n","cmp('bnmeani', dbnmeani, bnmeani)\n","cmp('hprebn', dhprebn, hprebn)\n","cmp('embcat', dembcat, embcat)\n","cmp('W1', dW1, W1)\n","cmp('b1', db1, b1)\n","cmp('emb', demb, emb)\n","cmp('C', dC, C)"],"metadata":{"id":"TtKiU_JgHPqG","executionInfo":{"status":"ok","timestamp":1746784256683,"user_tz":-540,"elapsed":54,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"dcf03a07-8736-411b-efd7-fe24096ea04d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["logprobs        | exact : True  | approximate : True  | maxdiff : 0.0\n","probs           | exact : True  | approximate : True  | maxdiff : 0.0\n","counts_sum_inv  | exact : True  | approximate : True  | maxdiff : 0.0\n","counts_sum      | exact : True  | approximate : True  | maxdiff : 0.0\n","counts          | exact : True  | approximate : True  | maxdiff : 0.0\n","norm_logits     | exact : True  | approximate : True  | maxdiff : 0.0\n","logit_maxes     | exact : True  | approximate : True  | maxdiff : 0.0\n","logits          | exact : True  | approximate : True  | maxdiff : 0.0\n","h               | exact : True  | approximate : True  | maxdiff : 0.0\n","W2              | exact : True  | approximate : True  | maxdiff : 0.0\n","b2              | exact : True  | approximate : True  | maxdiff : 0.0\n","hpreact         | exact : False | approximate : True  | maxdiff : 9.313225746154785e-10\n","bngain          | exact : False | approximate : True  | maxdiff : 1.862645149230957e-09\n","bnbias          | exact : False | approximate : True  | maxdiff : 7.450580596923828e-09\n","bnraw           | exact : False | approximate : True  | maxdiff : 9.313225746154785e-10\n","bnvar_inv       | exact : False | approximate : True  | maxdiff : 5.587935447692871e-09\n","bnvar           | exact : False | approximate : True  | maxdiff : 9.313225746154785e-10\n","bndiff2         | exact : False | approximate : True  | maxdiff : 2.9103830456733704e-11\n","bndiff          | exact : False | approximate : True  | maxdiff : 9.313225746154785e-10\n","bnmeani         | exact : False | approximate : True  | maxdiff : 3.725290298461914e-09\n","hprebn          | exact : False | approximate : True  | maxdiff : 9.313225746154785e-10\n","embcat          | exact : False | approximate : True  | maxdiff : 3.725290298461914e-09\n","W1              | exact : False | approximate : True  | maxdiff : 5.587935447692871e-09\n","b1              | exact : False | approximate : True  | maxdiff : 3.725290298461914e-09\n","emb             | exact : False | approximate : True  | maxdiff : 3.725290298461914e-09\n","C               | exact : False | approximate : True  | maxdiff : 1.4901161193847656e-08\n"]}]},{"cell_type":"markdown","source":["## Exercise - 2"],"metadata":{"id":"ix06dapJpeKk"}},{"cell_type":"code","source":["# Exercise 2: backprop through cross_entropy but all in one go\n","# to complete this challenge look at the mathematical expression of the loss,\n","# take the derivative, simplify the expression, and just write it out\n","\n","# forward pass\n","\n","# before:\n","# logit_maxes = logits.max(1, keepdim=True).values\n","# norm_logits = logits - logit_maxes # subtract max for numerical stability\n","# counts = norm_logits.exp()\n","# counts_sum = counts.sum(1, keepdims=True)\n","# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n","# probs = counts * counts_sum_inv\n","# logprobs = probs.log()\n","# loss = -logprobs[range(n), Yb].mean()\n","\n","# now:\n","loss_fast = F.cross_entropy(logits, Yb)\n","print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"],"metadata":{"id":"41sLE3aQeglk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746787477020,"user_tz":-540,"elapsed":12,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"ea164abe-a879-4897-ffcf-fb875ecd3636"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3.788137674331665 diff: 2.384185791015625e-07\n"]}]},{"cell_type":"code","source":["# backward pass\n","\n","# -----------------\n","# YOUR CODE HERE :)\n","dlogits = F.softmax(logits,1)\n","dlogits[range(n),Yb] -= 1\n","dlogits /= n\n","cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"],"metadata":{"id":"yaQ0jPhuegoX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746791955708,"user_tz":-540,"elapsed":24,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"ea73ed89-2167-43b9-e4e6-3310b88ee983"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["logits          | exact : False | approximate : True  | maxdiff : 9.313225746154785e-09\n"]}]},{"cell_type":"markdown","source":["## Exercise - 3"],"metadata":{"id":"QKqHfPrjphRc"}},{"cell_type":"code","source":["# Exercise 3: backprop through batchnorm but all in one go\n","# to complete this challenge look at the mathematical expression of the output of batchnorm,\n","# take the derivative w.r.t. its input, simplify the expression, and just write it out\n","# BatchNorm paper: https://arxiv.org/abs/1502.03167\n","\n","# forward pass\n","\n","# before:\n","# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n","# bndiff = hprebn - bnmeani\n","# bndiff2 = bndiff**2\n","# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n","# bnvar_inv = (bnvar + 1e-5)**-0.5\n","# bnraw = bndiff * bnvar_inv\n","# hpreact = bngain * bnraw + bnbias\n","\n","# now:\n","hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n","print('max diff:', (hpreact_fast - hpreact).abs().max())"],"metadata":{"id":"EiLdwB_oegqi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746796019729,"user_tz":-540,"elapsed":7,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"2520544f-f69c-4597-e85d-9e25afcb41f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"]}]},{"cell_type":"code","source":["# backward pass\n","\n","# before we had:\n","# dbnraw = bngain * dhpreact\n","# dbndiff = bnvar_inv * dbnraw\n","# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n","# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n","# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n","# dbndiff += (2*bndiff) * dbndiff2\n","# dhprebn = dbndiff.clone()\n","# dbnmeani = (-dbndiff).sum(0)\n","# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n","\n","# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n","# (you'll also need to use some of the variables from the forward pass up above)\n","\n","# -----------------\n","# YOUR CODE HERE :)\n","dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n","# -----------------\n","\n","cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"],"metadata":{"id":"qqpVGYX-egsz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746796059061,"user_tz":-540,"elapsed":25,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"3718d909-a2ed-4474-b056-db95087e2068"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["hprebn          | exact : False | approximate : True  | maxdiff : 9.313225746154785e-10\n"]}]},{"cell_type":"markdown","source":["## Exercise - 4"],"metadata":{"id":"ZZgWGXJPpvJS"}},{"cell_type":"code","source":["# Exercise 4: putting it all together!\n","# Train the MLP neural net with your own backward pass\n","\n","# init\n","n_embd = 10 # the dimensionality of the character embedding vectors\n","n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n","\n","g = torch.Generator().manual_seed(2147483647) # for reproducibility\n","C  = torch.randn((vocab_size, n_embd),            generator=g)\n","# Layer 1\n","W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n","b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n","# Layer 2\n","W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n","b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n","# BatchNorm parameters\n","bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n","bnbias = torch.randn((1, n_hidden))*0.1\n","\n","parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n","print(sum(p.nelement() for p in parameters)) # number of parameters in total\n","for p in parameters:\n","  p.requires_grad = True\n","\n","# same optimization as last time\n","max_steps = 200000\n","batch_size = 32\n","n = batch_size # convenience\n","lossi = []\n","\n","# use this context manager for efficiency once your backward pass is written (TODO)\n","with torch.no_grad():\n","\n","  # kick off optimization\n","  for i in range(max_steps):\n","\n","    # minibatch construct\n","    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n","    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n","\n","    # forward pass\n","    emb = C[Xb] # embed the characters into vectors\n","    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n","    # Linear layer\n","    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n","    # BatchNorm layer\n","    # -------------------------------------------------------------\n","    bnmean = hprebn.mean(0, keepdim=True)\n","    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n","    bnvar_inv = (bnvar + 1e-5)**-0.5\n","    bnraw = (hprebn - bnmean) * bnvar_inv\n","    hpreact = bngain * bnraw + bnbias\n","    # -------------------------------------------------------------\n","    # Non-linearity\n","    h = torch.tanh(hpreact) # hidden layer\n","    logits = h @ W2 + b2 # output layer\n","    loss = F.cross_entropy(logits, Yb) # loss function\n","\n","    # backward pass\n","    for p in parameters:\n","      p.grad = None\n","    #loss.backward() # use this for correctness comparisons, delete it later!\n","\n","    # manual backprop! #swole_doge_meme\n","    # YOUR CODE HERE :)\n","    # -----------------\n","    dlogits = F.softmax(logits, 1)\n","    dlogits[range(n), Yb] -= 1\n","    dlogits /= n\n","    # 2nd layer backprop\n","    dh = dlogits @ W2.T\n","    dW2 = h.T @ dlogits\n","    db2 = dlogits.sum(0)\n","    # tanh\n","    dhpreact = (1.0 - h**2) * dh\n","    # batchnorm backprop\n","    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n","    dbnbias = dhpreact.sum(0, keepdim=True)\n","    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n","    # 1st layer\n","    dembcat = dhprebn @ W1.T\n","    dW1 = embcat.T @ dhprebn\n","    db1 = dhprebn.sum(0)\n","    # embedding\n","    demb = dembcat.view(emb.shape)\n","    dC = torch.zeros_like(C)\n","    for k in range(Xb.shape[0]):\n","      for j in range(Xb.shape[1]):\n","        ix = Xb[k,j]\n","        dC[ix] += demb[k,j]\n","    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n","    # -----------------\n","\n","    # update\n","    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n","    for p, grad in zip(parameters, grads):\n","      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n","      p.data += -lr * grad # new way of swole doge TODO: enable\n","\n","    # track stats\n","    if i % 10000 == 0: # print every once in a while\n","      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n","    lossi.append(loss.log10().item())\n","\n","  #   if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n","  #     break"],"metadata":{"id":"OrshS_PpegvX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746797121955,"user_tz":-540,"elapsed":883899,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"b4929fa4-00bc-4c86-9471-8f03725b2809"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12297\n","      0/ 200000: 3.7737\n","  10000/ 200000: 2.1984\n","  20000/ 200000: 2.4065\n","  30000/ 200000: 2.4609\n","  40000/ 200000: 1.9743\n","  50000/ 200000: 2.4406\n","  60000/ 200000: 2.4105\n","  70000/ 200000: 2.0340\n","  80000/ 200000: 2.3485\n","  90000/ 200000: 2.1591\n"," 100000/ 200000: 2.0355\n"," 110000/ 200000: 2.3489\n"," 120000/ 200000: 1.9973\n"," 130000/ 200000: 2.4822\n"," 140000/ 200000: 2.3244\n"," 150000/ 200000: 2.1916\n"," 160000/ 200000: 1.9947\n"," 170000/ 200000: 1.8304\n"," 180000/ 200000: 1.9800\n"," 190000/ 200000: 1.9098\n"]}]},{"cell_type":"code","source":["# useful for checking your gradients\n","# for p,g in zip(parameters, grads):\n","#   cmp(str(tuple(p.shape)), g, p)"],"metadata":{"id":"mlAgky_qRIIw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calibrate the batch norm at the end of training\n","\n","# this is cuz we don't have running mean and std in the our training loop\n","\n","with torch.no_grad():\n","  # pass the training set through\n","  emb = C[Xtr]\n","  embcat = emb.view(emb.shape[0], -1)\n","  hpreact = embcat @ W1 + b1\n","  # measure the mean/std over the entire training set\n","  bnmean = hpreact.mean(0, keepdim=True)\n","  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"],"metadata":{"id":"wsPvvyhaegxl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# evaluate train and val loss\n","\n","@torch.no_grad() # this decorator disables gradient tracking\n","def split_loss(split):\n","  x,y = {\n","    'train': (Xtr, Ytr),\n","    'val': (Xdev, Ydev),\n","    'test': (Xte, Yte),\n","  }[split]\n","  emb = C[x] # (N, block_size, n_embd)\n","  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n","  hpreact = embcat @ W1 + b1\n","  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n","  h = torch.tanh(hpreact) # (N, n_hidden)\n","  logits = h @ W2 + b2 # (N, vocab_size)\n","  loss = F.cross_entropy(logits, y)\n","  print(split, loss.item())\n","\n","split_loss('train')\n","split_loss('val')"],"metadata":{"id":"5Mme7iTDegz8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746797807840,"user_tz":-540,"elapsed":3330,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"ba8f62ae-a63a-435a-890d-63629aee09e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train 2.0714874267578125\n","val 2.1100926399230957\n"]}]},{"cell_type":"code","source":["# sample from the model\n","g = torch.Generator().manual_seed(2147483647 + 10)\n","\n","for _ in range(20):\n","\n","    out = []\n","    context = [0] * block_size # initialize with all ...\n","    while True:\n","      # ------------\n","      # forward pass:\n","      # Embedding\n","      emb = C[torch.tensor([context])] # (1,block_size,d)\n","      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n","      hpreact = embcat @ W1 + b1\n","      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n","      h = torch.tanh(hpreact) # (N, n_hidden)\n","      logits = h @ W2 + b2 # (N, vocab_size)\n","      # ------------\n","      # Sample\n","      probs = F.softmax(logits, dim=1)\n","      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n","      context = context[1:] + [ix]\n","      out.append(ix)\n","      if ix == 0:\n","        break\n","\n","    print(''.join(itos[i] for i in out))"],"metadata":{"id":"EQG-C5d3eg26","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746797814958,"user_tz":-540,"elapsed":121,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"836383f1-f746-44f6-ed6c-ecabfb694524"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mora.\n","mayah.\n","seel.\n","ndhayla.\n","reimani.\n","jarlee.\n","adelyn.\n","elin.\n","shi.\n","jen.\n","eden.\n","sana.\n","arleigh.\n","malaia.\n","noshuberlyni.\n","jest.\n","jair.\n","jennex.\n","teron.\n","ububy.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Do1i2b1WHPsN"},"execution_count":null,"outputs":[]}]}