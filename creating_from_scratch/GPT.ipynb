{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMwrsr3a5s95DFUHSE0fx5/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Building GPT from scratch"],"metadata":{"id":"duOiaEUGvfCB"}},{"cell_type":"markdown","source":["## Import the dataset and stuff"],"metadata":{"id":"q0nYpe1YvfED"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"W22vUe0zverd","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1746886363683,"user_tz":-540,"elapsed":884,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"4b2c93ed-1d01-4a68-e99a-ba4332fc71f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-05-10 14:12:42--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: ‘input.txt’\n","\n","input.txt           100%[===================>]   1.06M  4.44MB/s    in 0.2s    \n","\n","2025-05-10 14:12:43 (4.44 MB/s) - ‘input.txt’ saved [1115394/1115394]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"]},{"cell_type":"code","source":["with open(\"input.txt\",\"r\",encoding='utf-8') as file :\n","    text = file.read()"],"metadata":{"id":"gqkl8e9gvfL7","executionInfo":{"status":"ok","timestamp":1746886367708,"user_tz":-540,"elapsed":3,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["print(\"length of dataset in characters : \",len(text))"],"metadata":{"id":"Q-ozOeuSvfOM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746886368399,"user_tz":-540,"elapsed":11,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"0fe994d4-efc4-4fc7-eeac-2e6123e2ed62"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["length of dataset in characters :  1115394\n"]}]},{"cell_type":"code","source":["print(text[:200])"],"metadata":{"id":"5t-T3wNRvfP8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746886368881,"user_tz":-540,"elapsed":9,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"c4b22e7f-d500-4ae1-dfb6-dd1dc74f0d6e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you\n"]}]},{"cell_type":"markdown","source":["## Starting up codes"],"metadata":{"id":"xm4D1b-rxGOr"}},{"cell_type":"code","source":["# here all the unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","print(''.join(chars))\n","print(vocab_size)"],"metadata":{"id":"ndq0SLkyvfTM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746886371616,"user_tz":-540,"elapsed":36,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"ecbb6424-74db-47ca-a373-c0f11bf47a07"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n","65\n"]}]},{"cell_type":"code","source":["# create a mapping from characters to integers\n","stoi = {ch:i for i,ch in enumerate(chars)}\n","itos = {i : ch for i,ch in enumerate(chars)}\n","encode = lambda s : [stoi[c] for c in s]    # encoder -- take a string, output a list of integers\n","decode = lambda l : ''.join(itos[i] for i in l)   # decoder -- take a list of integers, output a string\n","\n","print(encode(\"hii there\"))\n","print(decode(encode(\"hii there\")))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2J3XDKLEutIE","executionInfo":{"status":"ok","timestamp":1746886373996,"user_tz":-540,"elapsed":8,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"cff2b446-53aa-4f10-b199-4bf787dba021"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[46, 47, 47, 1, 58, 46, 43, 56, 43]\n","hii there\n"]}]},{"cell_type":"code","source":["# let's now encode the entire text dataset and store it into a torch.Tensor\n","import torch\n","data = torch.tensor(encode(text),dtype=torch.long)\n","print(data.shape,data.dtype)\n","print(data[:30])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3-VWZWCSutKt","executionInfo":{"status":"ok","timestamp":1746886380921,"user_tz":-540,"elapsed":5036,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"b99604a1-fa48-4de9-e372-807435493cc8"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1115394]) torch.int64\n","tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n","        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43])\n"]}]},{"cell_type":"code","source":["# Let's now split up the data into train and validation sets\n","n = int(0.9*len(data))\n","train_data = data[:n]\n","val_data = data[n:]"],"metadata":{"id":"DD6mfoO7utNS","executionInfo":{"status":"ok","timestamp":1746886386152,"user_tz":-540,"elapsed":53,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["block_size = 8   # umnuhiig boduul, ehnii character-aas l ehleed daraagiinhiiga predict hiigeed yavna.\n","train_data[:block_size +1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"05zrQcvoutPM","executionInfo":{"status":"ok","timestamp":1746886387525,"user_tz":-540,"elapsed":9,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"c7871498-6d9f-42fe-a2e2-b6ad98eadbde"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["x = train_data[:block_size]\n","y = train_data[1:block_size+1]\n","for t in range(block_size):\n","    context = x[:t+1]\n","    target = y[t]\n","    print(f\"When input is {context}, the target : {target}\")"],"metadata":{"id":"4jJ3UZVJvfVO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746886390320,"user_tz":-540,"elapsed":9,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"5cf35ae6-b618-404b-9a57-e2650f309c52"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["When input is tensor([18]), the target : 47\n","When input is tensor([18, 47]), the target : 56\n","When input is tensor([18, 47, 56]), the target : 57\n","When input is tensor([18, 47, 56, 57]), the target : 58\n","When input is tensor([18, 47, 56, 57, 58]), the target : 1\n","When input is tensor([18, 47, 56, 57, 58,  1]), the target : 15\n","When input is tensor([18, 47, 56, 57, 58,  1, 15]), the target : 47\n","When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target : 58\n"]}]},{"cell_type":"code","source":["# hyperparameters\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","torch.manual_seed(1337);\n","\n","n_embd = 384\n","batch_size = 64\n","block_size = 256  # what is the maximum context length for predictions?\n","max_iters = 5001\n","eval_interval = 500\n","learning_rate = 3e-4\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","eval_iters = 200\n","n_layer = 6\n","dropout = 0.2\n","n_head = 6"],"metadata":{"id":"VOFQfeQfVIF0","executionInfo":{"status":"ok","timestamp":1746886398404,"user_tz":-540,"elapsed":21,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(1337)\n","bo_size = 8\n","ba_size = 4\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == \"train\" else val_data\n","    ix = torch.randint(len(data) - bo_size,(ba_size,))\n","    x = torch.stack([data[i:i+bo_size] for i in ix])\n","    y = torch.stack([data[i+1 : i+bo_size+1] for i in ix])\n","    x,y = x.to(device), y.to(device)\n","    return x,y\n","\n","xb,yb = get_batch(\"train\")\n","print(\"Inputs : \")\n","print(xb.shape)\n","print(xb)\n","print(\"targets : \")\n","print(yb.shape)\n","print(yb)\n","\n","print(\"-------\")\n","for b in range(1):\n","    for t in range(bo_size):  # batch_size gej bas har.\n","        context = xb[b,:t+1]\n","        target = yb[b,t]\n","        print(f\"when input is {context.tolist()} the target : {target}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"_9QPHPTMxd4y","executionInfo":{"status":"ok","timestamp":1746886452374,"user_tz":-540,"elapsed":39,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"87d11567-593c-44f0-ff27-b7f36b9dd8d3"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Inputs : \n","torch.Size([4, 8])\n","tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n","        [44, 53, 56,  1, 58, 46, 39, 58],\n","        [52, 58,  1, 58, 46, 39, 58,  1],\n","        [25, 17, 27, 10,  0, 21,  1, 54]])\n","targets : \n","torch.Size([4, 8])\n","tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n","        [53, 56,  1, 58, 46, 39, 58,  1],\n","        [58,  1, 58, 46, 39, 58,  1, 46],\n","        [17, 27, 10,  0, 21,  1, 54, 39]])\n","-------\n","when input is [24] the target : 43\n","when input is [24, 43] the target : 58\n","when input is [24, 43, 58] the target : 5\n","when input is [24, 43, 58, 5] the target : 57\n","when input is [24, 43, 58, 5, 57] the target : 1\n","when input is [24, 43, 58, 5, 57, 1] the target : 46\n","when input is [24, 43, 58, 5, 57, 1, 46] the target : 43\n","when input is [24, 43, 58, 5, 57, 1, 46, 43] the target : 39\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"BRlI5GX7jVFD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n"],"metadata":{"id":"Hbz5YxswjVHb","executionInfo":{"status":"ok","timestamp":1746886548209,"user_tz":-540,"elapsed":5,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["x,y = get_batch(\"train\")"],"metadata":{"id":"ZJIa19KXjVJW","executionInfo":{"status":"ok","timestamp":1746886559165,"user_tz":-540,"elapsed":14,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["x.shape , y.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TnYPGQf4jPFh","executionInfo":{"status":"ok","timestamp":1746886565535,"user_tz":-540,"elapsed":48,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"6571fa5f-eb96-4f9e-989a-f554352f9d58"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([64, 256]), torch.Size([64, 256]))"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# estimate loss function\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in [\"train\",\"val\"]:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X,Y = get_batch(split)\n","            logits, loss = model(X,Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out"],"metadata":{"id":"Njne0MGdW596","executionInfo":{"status":"ok","timestamp":1746886467255,"user_tz":-540,"elapsed":3,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["### The Mathematical trick in self-attention\n","\n","### Trying masked self-attention"],"metadata":{"id":"kIw8FnW5Ptzl"}},{"cell_type":"code","source":["# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n","torch.manual_seed(42)\n","a = torch.tril(torch.ones(3,3))\n","a = a / torch.sum(a,1,keepdim=True)  # 3x1 size-tai row bolgon ni ter row-iinho sum-iig shaasan pisda baina.\n","b = torch.randint(0,10,(3,2)).float()\n","c = a @ b\n","print(\"a = \")\n","print(a)\n","print(\"b = \")\n","print(b)\n","print(\"c = \")\n","print(c)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l0iwWjY5PYYZ","executionInfo":{"status":"ok","timestamp":1746886471050,"user_tz":-540,"elapsed":104,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"df2072dc-86cd-45b5-b4ee-915ff5888a02"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["a = \n","tensor([[1.0000, 0.0000, 0.0000],\n","        [0.5000, 0.5000, 0.0000],\n","        [0.3333, 0.3333, 0.3333]])\n","b = \n","tensor([[2., 7.],\n","        [6., 4.],\n","        [6., 5.]])\n","c = \n","tensor([[2.0000, 7.0000],\n","        [4.0000, 5.5000],\n","        [4.6667, 5.3333]])\n"]}]},{"cell_type":"code","source":["# consider the following toy example:\n","\n","torch.manual_seed(1337)\n","B,T,C = 4,8,32 # batch, time, channels\n","x = torch.randn(B,T,C)\n","x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f3R13JwfRNlB","executionInfo":{"status":"ok","timestamp":1746881351596,"user_tz":-540,"elapsed":13,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"83b8c1e3-9757-42cc-a2d5-53eb866ec1c1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 32])"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["# We want x[b,t] = mean_{i<=t} x[b,i]\n","xbow = torch.zeros((B,T,C))\n","for b in range(B):\n","    for t in range(T):\n","        xprev = x[b,:t+1] # (t,C)\n","        xbow[b,t] = torch.mean(xprev, 0)\n","print(xbow.shape)\n","print(xprev.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hscnH129RNsy","executionInfo":{"status":"ok","timestamp":1746881351600,"user_tz":-540,"elapsed":3,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"640bf216-1ed0-46d6-d920-e6677f67c3e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 8, 32])\n","torch.Size([8, 32])\n"]}]},{"cell_type":"code","source":["# version 2: using matrix multiply for a weighted aggregation\n","wei = torch.tril(torch.ones(T, T))\n","wei = wei / wei.sum(1, keepdim=True)\n","xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n","xbow2.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yEkbv9KeRPJ4","executionInfo":{"status":"ok","timestamp":1746881351603,"user_tz":-540,"elapsed":2,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"4cb6c85c-09c2-440f-9c15-28b3e1df82dd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 32])"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["# version 3 : use Softmax\n","tril = torch.tril(torch.ones(T,T))\n","wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril==0 , float(\"-inf\"))\n","wei = F.softmax(wei,dim=-1)\n","xbow3 = wei @ x\n","torch.allclose(xbow2,xbow3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oTQCcSZRRPb9","executionInfo":{"status":"ok","timestamp":1746881351606,"user_tz":-540,"elapsed":2,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"f7a9519d-4a6e-4999-a3d1-26ed6f44f173"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["# version 4 : self-attention\n","torch.manual_seed(1337)\n","B,T,C = 4,8,32   # batch_size , time, channels\n","x = torch.randn(B,T,C)\n","\n","# let's see a single Head perform self-attention\n","head_size = 16\n","key = nn.Linear(C,head_size,bias=False)\n","query = nn.Linear(C,head_size,bias=False)\n","value = nn.Linear(C,head_size,bias=False)\n","\n","k = key(x)    # (B,T,16)\n","q = query(x)  # (B,T,16)\n","\n","wei = q @ k.transpose(-2,-1)   # (B,T,16)  @ (B,16,T) ---> (B,T,T)\n","\n","\n","tril = torch.tril(torch.ones(T,T))\n","wei = wei.masked_fill(tril==0, float(\"-inf\"))\n","wei = F.softmax(wei,dim=-1)\n","\n","v = value(x)\n","out = wei @ v\n","\n","out.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UwymcWkNn_-K","executionInfo":{"status":"ok","timestamp":1746881351663,"user_tz":-540,"elapsed":56,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"e56ba771-dc77-41fa-dd1c-60490ebb2c0e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 16])"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["## Self Attention Head"],"metadata":{"id":"LCElcM-pwark"}},{"cell_type":"code","source":["class Head(nn.Module):\n","    \"\"\" one head of self-attention \"\"\"\n","    def __init__(self,head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd,head_size,bias=False)   # (C,h) -> (384,64)\n","        self.query = nn.Linear(n_embd,head_size,bias=False)\n","        self.value = nn.Linear(n_embd,head_size,bias=False)\n","        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))   # (256,256)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self,x):\n","        B,T,C = x.shape  # (64, 256, 384)\n","        k = self.key(x)  # → (64, 256, 64)  -> (B,T,h)\n","        q = self.query(x)\n","        v = self.value(x)\n","\n","        # compute attention scores (\"affinities\")\n","        wei = q @ k.transpose(-2,-1) * C**-0.5  #  (B,T,h) @ (B,h,T) ---> (B,T,T)\n","        wei = wei.masked_fill(self.tril[:T,:T]==0,float(\"-inf\"))\n","        wei = F.softmax(wei,dim=-1)\n","        wei = self.dropout(wei)\n","        #perform the weighted aggregation of the values\n","        out = wei @ v  # (B,T,T) @ (B,T,h) --->  (B,T,h)\n","        return out    # → (64, 256, 64)"],"metadata":{"id":"9Khs_vCTRPtC","executionInfo":{"status":"ok","timestamp":1746887121146,"user_tz":-540,"elapsed":40,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["value = nn.Linear(n_embd,64,bias=False)\n","x = torch.randn((64,256,384))\n","value(x).shape   # each head outputs 64, and it will be concataned back to 384 . cuz of 6 multi-heads"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"82dAx_qoliU4","executionInfo":{"status":"ok","timestamp":1746887341334,"user_tz":-540,"elapsed":128,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"0109c4ee-25da-47cc-d23e-7cb5c889747a"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([64, 256, 64])"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["## Multi Head Attention"],"metadata":{"id":"8iTkeZRnCdq3"}},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    \"\"\"multiple heads of self-attention in parallel\"\"\"\n","    def __init__(self,num_heads,head_size):   # num_heads = 6 , head_size = 64\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(num_heads * head_size,n_embd)\n","\n","    def forward(self,x):\n","        out =  torch.cat([h(x) for h in self.heads],dim=-1)  #   6 * (64, 256, 64)  ---> (64, 256, 384)\n","        out = self.proj(out)\n","        return out   # -----> (64, 256, 384)"],"metadata":{"id":"JjRN_DCoyvUk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Feed Forward Network"],"metadata":{"id":"pdG_rG-jEKgg"}},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity\"\"\"\n","    def __init__(self,n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd,4*n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4*n_embd,n_embd),\n","            nn.Dropout(dropout)\n","        )\n","    def forward(self,x):\n","        return self.net(x)"],"metadata":{"id":"xNvAVsQXyvXB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Block"],"metadata":{"id":"xWN2AvTiFjs9"}},{"cell_type":"code","source":["class Block(nn.Module):\n","    \"\"\" Transformer block : communication followed by computation \"\"\"\n","    def __init__(self,n_embd,n_head):\n","        # n_embd : embedding dimension, n_head : the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head,head_size)\n","        self.ffwd = FeedForward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self,x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x    #  ---> (64, 256, 384)"],"metadata":{"id":"7ObK3sngFjz0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## BigramLanguageModel"],"metadata":{"id":"F68e06AJJT5j"}},{"cell_type":"code","source":["class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size,n_embd)\n","        self.blocks = nn.Sequential(*[Block(n_embd,n_head=n_head) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n","        self.lm_head = nn.Linear(n_embd,vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        B,T = idx.shape   # (64,256)\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        tok_embd = self.token_embedding_table(idx)   # (B,T,C)   # (64,265,384)\n","        pos_embd = self.position_embedding_table(torch.arange(T,device=device))  # (T,C)   # ingej loop hiij, vector-uudaa index-lej avjgaa genee. ternees busdaar bol, shineer uusgeegui.\n","        x = tok_embd + pos_embd   # (B,T,C)\n","        x = self.blocks(x)    # (B,T,C)   # (64,256,384)\n","        x = self.ln_f(x)\n","        logits = self.lm_head(x)   # (B,T,vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape    # (64,256,65)\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            #crop idx to the last block_size tokens\n","            idx_cond = idx[:,-block_size:]\n","            # get the predictions\n","            logits, loss = self(idx)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","\n","        return idx\n"],"metadata":{"id":"ZTsyainzxd6e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = BigramLanguageModel()\n","m = model.to(device)\n","\n","logits, loss = m(xb, yb)\n","print(logits.shape)\n","print(loss)\n","\n","# print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n","# # input just 1 number which is 0. and generate 100 chars. them pluck out the numbers from tensor,change it to list for decoding it.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mVhB0mICJYg8","executionInfo":{"status":"ok","timestamp":1746881352688,"user_tz":-540,"elapsed":1017,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"c790dea4-7270-471a-a956-4b63da3e1f02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16384, 65])\n","tensor(4.3299, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"code","source":["# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(m.parameters(),lr=1e-3)"],"metadata":{"id":"lMKDAG_wJbEZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"HUKA8qioJCmj"}},{"cell_type":"code","source":["for iter in range(max_iters):\n","\n","    # every once in a while evaluate the loss on the train and val sets\n","    if iter % eval_interval == 0:\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    # sample a batch of data\n","    xb,yb = get_batch(\"train\")\n","\n","    # evaluate the loss\n","    logits,loss = m(xb,yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x8M4V0_rFj1w","executionInfo":{"status":"ok","timestamp":1746882072578,"user_tz":-540,"elapsed":715287,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"871e6df0-6a06-4af2-ea4e-6cb5e1bc2cb7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["step 0: train loss 4.3246, val loss 4.3234\n","step 500: train loss 1.6636, val loss 1.8272\n","step 1000: train loss 1.3348, val loss 1.5689\n","step 1500: train loss 1.2071, val loss 1.5049\n","step 2000: train loss 1.1152, val loss 1.4949\n","step 2500: train loss 1.0300, val loss 1.5125\n","step 3000: train loss 0.9434, val loss 1.5635\n","step 3500: train loss 0.8519, val loss 1.6224\n","step 4000: train loss 0.7587, val loss 1.7002\n","step 4500: train loss 0.6730, val loss 1.7825\n","step 5000: train loss 0.5891, val loss 1.8790\n"]}]},{"cell_type":"markdown","source":["# Fullll Codeee !!!"],"metadata":{"id":"qpRHxt7bWF-J"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","# hyperparameters\n","batch_size = 16 # how many independent sequences will we process in parallel?\n","block_size = 32 # what is the maximum context length for predictions?\n","max_iters = 5000\n","eval_interval = 100\n","learning_rate = 1e-3\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200\n","n_embd = 64\n","n_head = 4\n","n_layer = 4\n","dropout = 0.0\n","# ------------\n","\n","torch.manual_seed(1337)\n","\n","# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","with open('input.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()\n","\n","# here are all the unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","# create a mapping from characters to integers\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","# Train and test splits\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","# data loading\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","class Head(nn.Module):\n","    \"\"\" one head of self-attention \"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B,T,C = x.shape\n","        k = self.key(x)   # (B,T,C)\n","        q = self.query(x) # (B,T,C)\n","        # compute attention scores (\"affinities\")\n","        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","        wei = F.softmax(wei, dim=-1) # (B, T, T)\n","        wei = self.dropout(wei)\n","        # perform the weighted aggregation of the values\n","        v = self.value(x) # (B,T,C)\n","        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n","        return out\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(n_embd, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out\n","\n","class FeedFoward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Block(nn.Module):\n","    \"\"\" Transformer block: communication followed by computation \"\"\"\n","\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedFoward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x\n","\n","# super simple bigram model\n","class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n","        x = tok_emb + pos_emb # (B,T,C)\n","        x = self.blocks(x) # (B,T,C)\n","        x = self.ln_f(x) # (B,T,C)\n","        logits = self.lm_head(x) # (B,T,vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self(idx_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n","\n","model = BigramLanguageModel()\n","m = model.to(device)\n","# print the number of parameters in the model\n","print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n","\n","# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"4fPuEYjFWGE0","executionInfo":{"status":"ok","timestamp":1746883422719,"user_tz":-540,"elapsed":327250,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"8af30481-f37a-44cf-b19c-40d90999c8df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.209729 M parameters\n","step 0: train loss 4.4116, val loss 4.4022\n","step 100: train loss 2.6568, val loss 2.6670\n","step 200: train loss 2.5090, val loss 2.5058\n","step 300: train loss 2.4197, val loss 2.4337\n","step 400: train loss 2.3501, val loss 2.3562\n","step 500: train loss 2.2965, val loss 2.3127\n","step 600: train loss 2.2412, val loss 2.2502\n","step 700: train loss 2.2053, val loss 2.2196\n","step 800: train loss 2.1631, val loss 2.1862\n","step 900: train loss 2.1233, val loss 2.1499\n","step 1000: train loss 2.1035, val loss 2.1306\n","step 1100: train loss 2.0688, val loss 2.1182\n","step 1200: train loss 2.0383, val loss 2.0801\n","step 1300: train loss 2.0240, val loss 2.0645\n","step 1400: train loss 1.9929, val loss 2.0366\n","step 1500: train loss 1.9699, val loss 2.0311\n","step 1600: train loss 1.9599, val loss 2.0447\n","step 1700: train loss 1.9396, val loss 2.0123\n","step 1800: train loss 1.9080, val loss 1.9932\n","step 1900: train loss 1.9080, val loss 1.9860\n","step 2000: train loss 1.8818, val loss 1.9941\n","step 2100: train loss 1.8714, val loss 1.9764\n","step 2200: train loss 1.8574, val loss 1.9599\n","step 2300: train loss 1.8526, val loss 1.9496\n","step 2400: train loss 1.8403, val loss 1.9420\n","step 2500: train loss 1.8145, val loss 1.9424\n","step 2600: train loss 1.8271, val loss 1.9403\n","step 2700: train loss 1.8112, val loss 1.9291\n","step 2800: train loss 1.8040, val loss 1.9210\n","step 2900: train loss 1.8043, val loss 1.9287\n","step 3000: train loss 1.7937, val loss 1.9174\n","step 3100: train loss 1.7678, val loss 1.9148\n","step 3200: train loss 1.7543, val loss 1.9102\n","step 3300: train loss 1.7547, val loss 1.9030\n","step 3400: train loss 1.7541, val loss 1.8932\n","step 3500: train loss 1.7367, val loss 1.8976\n","step 3600: train loss 1.7235, val loss 1.8854\n","step 3700: train loss 1.7267, val loss 1.8813\n","step 3800: train loss 1.7199, val loss 1.8882\n","step 3900: train loss 1.7217, val loss 1.8716\n","step 4000: train loss 1.7121, val loss 1.8559\n","step 4100: train loss 1.7101, val loss 1.8755\n","step 4200: train loss 1.7061, val loss 1.8631\n","step 4300: train loss 1.7001, val loss 1.8465\n","step 4400: train loss 1.7049, val loss 1.8616\n","step 4500: train loss 1.6904, val loss 1.8478\n","step 4600: train loss 1.6833, val loss 1.8277\n","step 4700: train loss 1.6808, val loss 1.8452\n","step 4800: train loss 1.6695, val loss 1.8453\n","step 4900: train loss 1.6701, val loss 1.8369\n","step 4999: train loss 1.6616, val loss 1.8201\n","\n","\n","KING RICHAND II:\n","Shall by become to musbe doest thrust the gate\n","My art that usque, God?\n","\n","MEXENES:\n","Butwere my feanst, I zormur\n","Yourselfom in heart mile dill, at misters, I in latient,\n","Worsts, and the now our was twells no me upolds;\n","Hond my sprunt as speak you: none\n","In Boyanterioly home.\n","Who like agaion,---And thee, by we still,\n","His The shience poor of his but\n","that nobrurtef so;\n","Angint my monte in excations, Pried my of.\n","\n","HENRY BOLINGBROY::\n","Saday Warwick to Bauintchir accanny, rents I am you!\n","My fireass, I may.\n","And your gament so a cempres-ennome.\n","\n","GLOUCESTER:\n","Your may in son thee, bod, with confessy.\n","Which migh.\n","\n","ANCAMrown:\n","My when.\n","\n","LARIAPNA:\n","Well, to imdut?\n","\n","LUCIO:\n","For it?\n","Youse so upon surre eRpetALE:\n","What's I have nows: will hear news our house,\n","Havake but fravius wran some. Do selsemmader scolly not\n","To yourself I surre desing\n","My sirre's med Cadius\n","festing of time the shows tears man ip you ou what that that saw\n","Of Becoln madon so hand reford.\n","But I love, wout is forcest, in ginvers road,\n","And you all, I hursend; go wher tcough, do did titance and to you or love\n","As be rews mines so men: by not, it-sonfitgrant of word,\n","And a borrow\n","Her his enIndred a canity to may ince anysing.\n","\n","LADY GAUMET:\n","Look, Reingue, man't my have eath I die;\n","Save thou for Henry fampertle; I\n","Have my of thee tremes deed of through he crown,\n","Nor be nie is clity beloser Wonduntal on the have beguar\n","Good vonsure to you, must a becamen them the deenust om Three;\n","Thy matter! How scowe you,\n","But wolldly tondering; I sokes soul be\n","the shaltesss;\n","River thou a-latscess:\n","Out.\n","\n","PORABELLA:\n","My, Boring strar\n","Egve waced being have Lary.\n","\n","BUMNTANUS:\n","\n","'Twas may-marrishn. Caw't,\n","Garity give affive you his, by assince;\n","I'll his shaw well rivoid: the wife\n","Onger with I have I divoilt, of munders.\n","\n","PLAUY:\n","Hou twhere with you\n","Talk'd our twould you her, to life he pusint\n","It. Titters, and were in of not your your son!\n","Nor am-nsent, Joar at ween of to,\n","Thre Riparding: I wills men's fortundeet,\n","Ame of the viirtuou\n","all statel abbel to the birth to croth stingling I Gramilign thing;\n","And dead me to thee before connurdererisHords are I'll told.\n","If my liest, and, broth\n","sorme would sincise: why death! God's but the optrange your your many\n","his med and ne'er, no not, care\n","Annds my someds not; my wints go yoursay.\n","\n","NORDIZANUO:\n","Yet while;\n","So marry upon il his been, live is me, wonst da my son,\n","that do he doung I ress patius;\n","Your enague should movence aste-timal\n","Trought that God, hor densless not;\n","As most; queech seeme\n","Misel, lible imbraid, to yet stand,\n","theirs is trurness away, and thou drue\n","And my chooly being Roman, for that shall to: where his norber rump matter you,\n","Who sweet not forth goty me would sgeen,\n","To last needs them set it! this you your bearth,\n","And there is me that bity the face here buty gates\n","I was have and Bacleingmery, is Dolk they graveight:\n","I did conteremp; I benam you.\n","\n","Sourd CABETH:\n","That like thee, hide, bruy as gruband\n","Fixs your kinteds by to Rightard:\n","Nor forth'd, Is I rave, what is throne.\n","\n","COMEONIUS:\n","Is faithwel my lord. Much quing what not incly been mead. God must at cause:\n","Outh in, I know with these dogn?\n","\n","RERCHARD II:\n","No, you throw times, ah my o'ety\n","Felrsed allorm lets bestile haves\n","To sleepessman that words? what you whice, cause your rancians,\n","Thats my leept was is the much; bust like you me.\n","\n","DUKENTIO:\n","Majorthy's father, sorrow? There;\n","Hew I peresed that that mochy on two see uncontrancees,\n","Hen, and, and than I have the care gavexpe to is at I'll wondsh,\n","Stay's thumb nothing realoidintence, or with not.\n","\n","BUCKINGEY:\n","How it\n","Tently this of the read mant the love of boward\n","Our vilighter to growns, this behatter wiltent.\n","\n","VORTEOP:\n","Madule:\n","beter thou aspoub'd she worthy sword\n","Moried to beeneasue, yet night do seekss your ear the said\n","Inted tents the sbitterf; my son\n","A thiquest our dead!\n","Againt!\n","Her you dest woulder of you\n","That thus of on cigntlement untont thou here thou country\n","come town are controusing bot! Ourse, am you,\n","Would surpent was down, in with thy why.\n","\n","COMIOLYCULENE:\n","That not you Cannot, set that like, if the like suppove?\n","\n","SenvALET:\n","This seeps, not that my seeds\n","hall screts I some. I is All thee thly hate bey,\n","But as any hose gauet tut, disham dely stroyany, out,\n","Is let's nober sween beling is outcr claw why we both out.\n","\n","PUTUS:\n","The very hastinglingnary\n","Your fity'l doth as is lifedge, good nour sure\n","but know's is it guilt, my if the buttood my wagges of his\n","Both loves your wondsuit have at.\n","\n","BOWIDIRY:\n","I the rummord yoursh.\n","\n","KING RICHARD:\n","Must I not, I I regain to mest him\n","Duke, death alf Decub it? my neeks flife abirty it.\n","\n","KING EDWBY:\n","Comes so sweet so sabsureld so, to this.\n","\n","Shoubethord.\n","\n","MENENIUS:\n","Not so?\n","\n","\n","COMINIUSH:\n","Nurse:\n","What, shame as live who discontreot\n","More, diely my wroses, I ill doth iount\n","And lant-dirry-py the upon town astrumshame;\n","Than where I bece you rumusts his forther?\n","Do my gresaty merbs, and which heart time would have\n","Tybress, being that, that poor this..\n","\n","GLOUS:\n","How-Kinging hat's the volio, I diets: threse they joy;\n","What is in turse in that kngemons,\n","Thou will she togund your bratiets horm'd that\n","-fattly, courselfaces\n","A bect untopy at our suchts colditore:\n","Nor this sacket of my mursed? Now, a brines.\n","Pond, as banider, the fear; goes in't,\n","Our from that him self the\n","Is parth\n","Fantless trumners;\n","Of the will his in an man, cread\n","Thigh on some facer of thy drost your hate;\n","Untruct to Firstels, his does,--\n","That his our say strive not\n","you mark his and agle thee! dultury, will mine one that your torse\n","As flongto\n","Whyle arwien that wish nor gaway surelves to honed. Goodly's then ears.\n","Lord the most cans, it? Juliet the dots,\n","In youUKEd that may noblemans fool.\n","Now sheek and riving good.\n","\n","BUCKINGHAMIU:\n","Commund him colits! indly Benoruly of anTy it.\n","Nor 'twerecond mety and gland,\n","Romans, wars, your pote-derself.\n","Mylest this gonstrowments twich heart own,\n","Coman, as I have will not gacefore what hate;\n","Sohar be one that.\n","Ne 'shear thon to appet thoughn saw this:\n","and King the patch, suchip the this withont race death,\n","But detumners I holde; then agent;\n","Me, this berforginamio stoomes was now:\n","Murder his bruttam me.\n","\n","LEONTES:\n","Estentrays, you, that trough\n","Hast flife own the give.\n","\n","Sclond:\n","\n","DUKE CUMINGES:\n","Lord, not firsts.\n","\n","HORTENS:\n","Sistre friend elsed: is thing\n","Uut, such of you that you as all while;\n","Pravour of virget, you nurtued old mist they him: hope;\n","And but bring you saud, gatiner obs?\n","'Princefe, Tent, where if him.\n","\n","COMINIUS:\n","Am us nou aman.\n","\n","CLARENCE:\n","Fall this be, speak: thou? go have what I had not.\n","\n","POLIXS:\n","He putrovant, her! and frields, in my and ronleving\n","With this besease and which hatands failunts\n","bed, wrong I smily woft. What mest, and beauth.\n","\n","PORINGHUP OF YORK:\n","And him there it.\n","Shall,\n","Gory sprother of our't; gaints how,\n","Joy the seal that arands.\n","\n","Thou shames.\n","\n","PORINCE:\n","Make and you would thou\n","At samile your hast madam\n","Toginings: eithres dose rust go have\n","For the scotes babut every acforts to your told this remalliing:\n","I will thee the worthy alting to repare us themselvosy: We what give\n","I layay of I man sistreast him boy taluce?\n","Now whrong: most pown; honour self\n","kindst were come; put is hing; they assignies\n","Him-must bringlish'ders, you\n","'Thow with is friar many sagace to their beir sold,\n","Tell see to me a good I not, brate\n","Thou of begand our thou that lived them heaven.\n","It yields, end, and you thy yet.\n","\n","HERRY PAREY:\n","But aauth, my have too his amide curation flather.\n","\n","POHUzy, I have, 'aut thout upon to more against, whiless.\n","I never'd the beamons beggrant, o' the came convoseman:\n","If these they from fibate, of make mine,\n","And yough let then becarsira that in Ammioly:\n","That bloyans part, shall force.\n","\n","KING RICHARD II:\n","You cyurse I'll not\n","and sunty, I truke, pardince the clovest,\n","Beriaget. O't some torny theirs' mishering, those in to seep.\n","What lomme thyself, to Hew to is as as honour thee,\n","Alterious brine, but stand-froth, if he will you, fray, in good out,\n","An have and toer; in with what make volish vone; for bervioline mosns,\n","To this have seemen hon\n"]}]},{"cell_type":"code","source":["\n","for iter in range(max_iters):\n","\n","    # every once in a while evaluate the loss on train and val sets\n","    if iter % eval_interval == 0 or iter == max_iters - 1:\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = model(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_K43L_r3WGG0","executionInfo":{"status":"ok","timestamp":1746884573715,"user_tz":-540,"elapsed":261294,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"e80263a5-90e6-4c7c-c369-6f517b30fb73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["step 0: train loss 1.5164, val loss 1.6936\n","step 100: train loss 1.5227, val loss 1.6986\n","step 200: train loss 1.5153, val loss 1.7047\n","step 300: train loss 1.5221, val loss 1.7085\n","step 400: train loss 1.5113, val loss 1.6979\n","step 500: train loss 1.5129, val loss 1.7072\n","step 600: train loss 1.5062, val loss 1.7093\n","step 700: train loss 1.5101, val loss 1.6932\n","step 800: train loss 1.5126, val loss 1.7024\n","step 900: train loss 1.5242, val loss 1.7203\n","step 1000: train loss 1.4974, val loss 1.7110\n","step 1100: train loss 1.5074, val loss 1.7045\n","step 1200: train loss 1.5117, val loss 1.6946\n","step 1300: train loss 1.5025, val loss 1.6910\n","step 1400: train loss 1.5103, val loss 1.7055\n","step 1500: train loss 1.5055, val loss 1.7292\n","step 1600: train loss 1.5063, val loss 1.7018\n","step 1700: train loss 1.5224, val loss 1.6941\n","step 1800: train loss 1.5107, val loss 1.7131\n","step 1900: train loss 1.5021, val loss 1.6824\n","step 2000: train loss 1.5035, val loss 1.7135\n","step 2100: train loss 1.5117, val loss 1.7082\n","step 2200: train loss 1.5066, val loss 1.6868\n","step 2300: train loss 1.5086, val loss 1.7008\n","step 2400: train loss 1.4973, val loss 1.6891\n","step 2500: train loss 1.5055, val loss 1.7007\n","step 2600: train loss 1.5083, val loss 1.6999\n","step 2700: train loss 1.5081, val loss 1.6890\n","step 2800: train loss 1.5128, val loss 1.6759\n","step 2900: train loss 1.4925, val loss 1.6954\n","step 3000: train loss 1.4994, val loss 1.6793\n","step 3100: train loss 1.4934, val loss 1.6728\n","step 3200: train loss 1.5007, val loss 1.6853\n","step 3300: train loss 1.4946, val loss 1.6698\n","step 3400: train loss 1.4966, val loss 1.6754\n","step 3500: train loss 1.4967, val loss 1.6914\n","step 3600: train loss 1.4924, val loss 1.6874\n","step 3700: train loss 1.4982, val loss 1.6830\n","step 3800: train loss 1.5038, val loss 1.6922\n","step 3900: train loss 1.4897, val loss 1.6999\n","step 4000: train loss 1.4995, val loss 1.6855\n","step 4100: train loss 1.4901, val loss 1.6745\n","step 4200: train loss 1.5026, val loss 1.6697\n","step 4300: train loss 1.4887, val loss 1.6802\n","step 4400: train loss 1.4882, val loss 1.6632\n","step 4500: train loss 1.4896, val loss 1.6804\n","step 4600: train loss 1.4985, val loss 1.6876\n","step 4700: train loss 1.4865, val loss 1.6772\n","step 4800: train loss 1.5039, val loss 1.6849\n","step 4900: train loss 1.4823, val loss 1.6693\n","step 4999: train loss 1.4980, val loss 1.6793\n"]}]},{"cell_type":"code","source":["# generate from the model\n","context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PZ9g1wrMyvZn","executionInfo":{"status":"ok","timestamp":1746884667430,"user_tz":-540,"elapsed":16686,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"27e7dcc4-e2c4-4788-a19e-f8acf7ce8533"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","PAULINA:\n","Unto wint. My lord is noble: heaven Warwick to o' the shall now than the worlsel:\n","These most reasoned maid from know,\n","And is a boughness sigh murders:\n","Where is drumst to his words.\n","\n","ANGELO:\n","Say, my name eye say soes\n","It is dilip amons, will go have: I do, he dagry,\n","That bholds him the fastly but of two desire to love,\n","His help.\n","\n","NORFOLK:\n","Say, sir, likecing of his kell or unlayable\n","And Citizen:\n","Where, like him\n","Against the sile him: it is no gentled\n","Being his redressess.\n","\n","DUKE VINCENTIO:\n","O, my man is?\n","You more on times about the villain\n","Was in her O.\n","\n","OXFORD:\n","Why, dare my smily bring soul, thou well.\n","\n","MENENIUS:\n","Sirry:\n","Less brother but sons her well-king, but\n","to marry consento thun of kning;\n","With, gently and you will to his queen will,\n","In all pagain it, and askeUngel a was to\n","to him, I have majesty talk! Was more I'll uselish in place have boy!\n","On shall near weapen; thou, men\n","your fathio, on and in rapnised,\n","Whene'er is, sir, Catesby, and shing yet,\n","With a witdness to part of aly Come,\n","Ay, How he do and you:--\n","put sostrike of you?\n","\n","Good Sen,\n","Grumbling Keen: Gentleman, go by me to\n","Bilthough tale own with everland,\n","Would balved to when? but am all take him,\n","Away he so woe hold last, ker?\n","\n","GRENOT:\n","Rake a mean, arm is deniech warry castled should be.\n","\n","DUKE VINCENTIO:\n","Ah, my let to bitten him with transper me,\n","To much Camentain the fetter vipity:\n","Take ostrunks to make the jades, or comexon much falsining,\n","Say not myself so. What should I close Alle!\n","\n","First Cityger:\n","My nagainst army.\n","\n","CLAUDIO:\n","His voices, to forgot, my cop: for marriage sheep his sbout other\n","MoIst brief my kny these Tower,\n","Young descutios! Welce, whose me on eaches good or witness;\n","Citizens; but me to me his suffer,\n","With therefore heart us counsel well: not not, fait.\n","He has is, restay ome of you, if You\n","fair blagm'd in deep.\n","Your like no but\n","A door guises woe words, Paul I demand;\n","Cate thee! Thus friend sun corneres\n","Womerving and in day, I speak it is their import, where Warwick tears are your ost;\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"gvMpPXXbXo5H"},"execution_count":null,"outputs":[]}]}