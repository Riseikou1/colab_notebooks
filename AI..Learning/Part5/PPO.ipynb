{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMDZsOc/r+ONSOKxEJJv0ga"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Proximal Policy Optimization"],"metadata":{"id":"D62cHmZBuUc6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJiAkp5ZuFyF"},"outputs":[],"source":["# Proximal Policy Optimization (PPO)\n","\n","# Check out this link for the complete model explanation: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n","\n","# Importing the libraries\n","\n","import gymnasium as gym\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import torch.nn.functional as F\n","from torch.distributions import MultivariateNormal\n","\n","# Setting the hyperparameters\n","\n","hidden_dim = 256\n","lr_actor = 3e-4\n","lr_critic = 1e-3\n","gamma = 0.99\n","gae_lambda = 0.95\n","ppo_epochs = 10\n","mini_batch_size = 64\n","ppo_clip = 0.2\n","buffer_size = 2048\n","update_timestep = buffer_size\n","action_std = 0.5  # Standard deviation for action exploration\n","\n","# Building the Actor-Critic Network\n","\n","class ActorCritic(nn.Module):\n","\n","    def __init__(self, num_actions):\n","        super(ActorCritic, self).__init__()\n","        # Common layers\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n","        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n","        self.fc1 = nn.Linear(64 * 7 * 7, hidden_dim)\n","        # Actor layers\n","        self.fc_actor = nn.Linear(hidden_dim, num_actions)\n","        self.log_std = nn.Parameter(torch.zeros(num_actions))\n","        # Critic layers\n","        self.fc_critic = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, state):\n","        x = F.relu(self.conv1(state))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        # Actor\n","        action_mean = self.fc_actor(x)\n","        action_var = torch.exp(self.log_std.expand_as(action_mean))\n","        cov_mat = torch.diag_embed(action_var) # covariance matrix..\n","        # Critic\n","        value = self.fc_critic(x)\n","        return action_mean, cov_mat, value\n","\n","# Implementing the Memory Buffer\n","\n","class Memory:\n","\n","    def __init__(self):\n","        self.actions = []\n","        self.states = []\n","        self.logprobs = []\n","        self.rewards = []\n","        self.is_terminals = []\n","\n","    def clear_memory(self):\n","        del self.actions[:]\n","        del self.states[:]\n","        del self.logprobs[:]\n","        del self.rewards[:]\n","        del self.is_terminals[:]\n","\n","# Building the PPO Agent\n","\n","class PPO:\n","\n","    def __init__(self, num_actions):\n","        self.policy = ActorCritic(num_actions)\n","        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr_actor)\n","        self.policy_old = ActorCritic(num_actions)\n","        self.policy_old.load_state_dict(self.policy.state_dict())  # Copies the weights and biases from self.policy to self.policy_old\n","        self.MseLoss = nn.MSELoss()\n","\n","    def select_action(self, state):\n","        with torch.no_grad():\n","            action_mean, action_var, _ = self.policy_old(state)\n","            dist = MultivariateNormal(action_mean, action_var)\n","            action = dist.sample()\n","            action_logprob = dist.log_prob(action)\n","        return action.detach().numpy(), action_logprob.detach()\n","\n","    def update(self, memory):\n","        # Monte Carlo estimate of state rewards:\n","        rewards = []\n","        discounted_reward = 0\n","        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n","            if is_terminal:\n","                discounted_reward = 0\n","            discounted_reward = reward + (gamma * discounted_reward)\n","            rewards.insert(0, discounted_reward)\n","        # Normalizing the rewards\n","        rewards = torch.tensor(rewards, dtype=torch.float32)\n","        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n","        # Converting list to tensor\n","        old_states = torch.stack(memory.states).detach()\n","        old_actions = torch.stack(memory.actions).detach()\n","        old_logprobs = torch.stack(memory.logprobs).detach()\n","        # Optimizing policy for K epochs\n","        for _ in range(ppo_epochs):\n","            # Evaluating old actions and values\n","            action_means, action_vars, state_values = self.policy(old_states)\n","            dists = MultivariateNormal(action_means, action_vars)\n","            logprobs = dists.log_prob(old_actions)\n","            dist_entropy = -logprobs.mean()\n","            # Finding the ratio (pi_theta / pi_theta__old)\n","            ratios = torch.exp(logprobs - old_logprobs.detach())\n","            # Finding Surrogate Loss\n","            advantages = rewards - state_values.detach()\n","            surr1 = ratios * advantages\n","            surr2 = torch.clamp(ratios, 1-ppo_clip, 1+ppo_clip) * advantages\n","            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n","            # Taking gradient step\n","            self.optimizer.zero_grad()\n","            loss.mean().backward()\n","            self.optimizer.step()\n","        # Copying new weights into old policy\n","        self.policy_old.load_state_dict(self.policy.state_dict())\n","\n","# Preprocessing the states\n","\n","def preprocess_state(state):\n","    state = np.ascontiguousarray(state, dtype=np.float32) / 255\n","    state = torch.from_numpy(state)\n","    return state.unsqueeze(0)\n","\n","# Setting up the environment\n","\n","env = gym.make('CarRacing-v2')\n","\n","# Creating the memory\n","\n","memory = Memory()\n","\n","# Creating the agent\n","\n","ppo = PPO(env.action_space.shape[0])\n","\n","# Implementing the Training Loop\n","\n","state = env.reset()\n","state = preprocess_state(state)\n","for t in range(1, update_timestep+1):\n","    action, action_logprob = ppo.select_action(state)\n","    next_state, reward, done, _ = env.step(action)\n","    next_state = preprocess_state(next_state)\n","    memory.states.append(state)\n","    memory.actions.append(torch.tensor(action))\n","    memory.logprobs.append(action_logprob)\n","    memory.rewards.append(reward)\n","    memory.is_terminals.append(done)\n","    state = next_state\n","    if done:\n","        state = env.reset()\n","        state = preprocess_state(state)\n","    if t % update_timestep == 0:\n","        ppo.update(memory)\n","        memory.clear_memory()\n"]}]}