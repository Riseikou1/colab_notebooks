{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNLzLHPcoR35Ey5zKCHy7f6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install gymnasium\n","!pip install swig\n","!pip install gymnasium[box2d]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kAuK_trPVxAA","executionInfo":{"status":"ok","timestamp":1731833985157,"user_tz":-540,"elapsed":73710,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"c88c43ce-ff93-4a7b-a300-6b5f9687ef3c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium\n","  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n","Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n","Collecting swig\n","  Downloading swig-4.2.1.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n","Downloading swig-4.2.1.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: swig\n","Successfully installed swig-4.2.1.post0\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (3.1.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.1)\n","Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.1.post0)\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376101 sha256=83f35a51a38df7ebd4330cc7efcf9c9a05d5d1b97038666c1d110845c43475ce\n","  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.5\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":348},"id":"7mp5KK9XVZu9","executionInfo":{"status":"error","timestamp":1731834000431,"user_tz":-540,"elapsed":15280,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"40426586-a6f4-4501-863d-06a5571f0492"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-fc83f2f2d4cd>\u001b[0m in \u001b[0;36m<cell line: 158>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_timestep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_logprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-fc83f2f2d4cd>\u001b[0m in \u001b[0;36mpreprocess_state\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."]}],"source":["# Proximal Policy Optimization (PPO)\n","\n","# Check out this link for the complete model explanation: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n","\n","# Importing the libraries\n","\n","import gymnasium as gym\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import torch.nn.functional as F\n","from torch.distributions import MultivariateNormal\n","\n","# Setting the hyperparameters\n","\n","hidden_dim = 256\n","lr_actor = 3e-4\n","lr_critic = 1e-3\n","gamma = 0.99\n","gae_lambda = 0.95\n","ppo_epochs = 10\n","mini_batch_size = 64\n","ppo_clip = 0.2\n","buffer_size = 2048\n","update_timestep = buffer_size\n","action_std = 0.5  # Standard deviation for action exploration\n","\n","# Building the Actor-Critic Network\n","\n","class ActorCritic(nn.Module):\n","\n","    def __init__(self, num_actions):\n","        super(ActorCritic, self).__init__()\n","        # Common layers\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n","        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n","        self.fc1 = nn.Linear(64 * 7 * 7, hidden_dim)\n","        # Actor layers\n","        self.fc_actor = nn.Linear(hidden_dim, num_actions)\n","        self.log_std = nn.Parameter(torch.zeros(num_actions)) # ingeed shaahaar learnable parameter gesen ug ym biana.\n","                # optimizer ene sdaf ni update hhiij chadna.represents uncertainty in distribution.\n","        # Critic layers\n","        self.fc_critic = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, state):\n","        x = F.relu(self.conv1(state))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        # Actor\n","        action_mean = self.fc_actor(x)\n","        action_var = torch.exp(self.log_std.expand_as(action_mean))\n","        cov_mat = torch.diag_embed(action_var)\n","        # Critic\n","        value = self.fc_critic(x)\n","        return action_mean, cov_mat, value\n","\n","# Implementing the Memory Buffer\n","\n","class Memory:\n","\n","    def __init__(self):\n","        self.actions = []\n","        self.states = []\n","        self.logprobs = []\n","        self.rewards = []\n","        self.is_terminals = []\n","\n","    def clear_memory(self):\n","        del self.actions[:]\n","        del self.states[:]\n","        del self.logprobs[:]\n","        del self.rewards[:]\n","        del self.is_terminals[:]\n","\n","# Building the PPO Agent\n","\n","class PPO:\n","\n","    def __init__(self, num_actions):\n","        self.policy = ActorCritic(num_actions)\n","        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr_actor)\n","        self.policy_old = ActorCritic(num_actions)  # nuguu clip method geed shaagaad sbsn sdagiin tuluu.\n","           # update hiisen weigth ntr ni std nter ni aimr hol zurkun tuld iim yum hiigeed baigaa. old_policy in yurn experience l gesen ug.\n","        self.policy_old.load_state_dict(self.policy.state_dict())\n","        self.MseLoss = nn.MSELoss()\n","\n","    def select_action(self, state):\n","        with torch.no_grad():\n","            action_mean, action_var, _ = self.policy_old(state)\n","            dist = MultivariateNormal(action_mean, action_var)\n","            action = dist.sample()\n","            action_logprob = dist.log_prob(action)\n","        return action.detach().numpy(), action_logprob.detach()\n","\n","    def update(self, memory):\n","        # Monte Carlo estimate of state rewards:\n","        rewards = []\n","        discounted_reward = 0\n","        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n","            if is_terminal:\n","                discounted_reward = 0\n","            discounted_reward = reward + (gamma * discounted_reward)\n","            rewards.insert(0, discounted_reward)\n","        # Normalizing the rewards\n","        rewards = torch.tensor(rewards, dtype=torch.float32)\n","        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n","        # Converting list to tensor\n","        old_states = torch.stack(memory.states).detach()\n","        old_actions = torch.stack(memory.actions).detach()\n","        old_logprobs = torch.stack(memory.logprobs).detach()\n","        # Optimizing policy for K epochs\n","        for _ in range(ppo_epochs):\n","            # Evaluating old actions and values\n","            action_means, action_vars, state_values = self.policy(old_states)\n","            dists = MultivariateNormal(action_means, action_vars)\n","            logprobs = dists.log_prob(old_actions)\n","            dist_entropy = -logprobs.mean()\n","            # Finding the ratio (pi_theta / pi_theta__old)\n","            ratios = torch.exp(logprobs - old_logprobs.detach())\n","            # Finding Surrogate Loss\n","            advantages = rewards - state_values.detach()\n","            surr1 = ratios * advantages\n","            surr2 = torch.clamp(ratios, 1-ppo_clip, 1+ppo_clip) * advantages\n","            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n","            # Taking gradient step\n","            self.optimizer.zero_grad()\n","            loss.mean().backward()\n","            self.optimizer.step()\n","        # Copying new weights into old policy\n","        self.policy_old.load_state_dict(self.policy.state_dict())\n","\n","# Preprocessing the states\n","\n","def preprocess_state(state):\n","    state = np.ascontiguousarray(state, dtype=np.float32) / 255\n","    state = torch.from_numpy(state)\n","    return state.unsqueeze(0)\n","\n","# Setting up the environment\n","\n","env = gym.make('CarRacing-v3')\n","\n","# Creating the memory\n","\n","memory = Memory()\n","\n","# Creating the agent\n","\n","ppo = PPO(env.action_space.shape[0])\n","\n","# Implementing the Training Loop\n","\n","state = env.reset()\n","state = preprocess_state(state)\n","for t in range(1, update_timestep+1):\n","    action, action_logprob = ppo.select_action(state)\n","    next_state, reward, done, _ = env.step(action)\n","    next_state = preprocess_state(next_state)\n","    memory.states.append(state)\n","    memory.actions.append(torch.tensor(action))\n","    memory.logprobs.append(action_logprob)\n","    memory.rewards.append(reward)\n","    memory.is_terminals.append(done)\n","    state = next_state\n","    if done:\n","        state = env.reset()\n","        state = preprocess_state(state)\n","    if t % update_timestep == 0:\n","        ppo.update(memory)\n","        memory.clear_memory()"]},{"cell_type":"code","source":[],"metadata":{"id":"nJHB5cdAVeiV"},"execution_count":null,"outputs":[]}]}