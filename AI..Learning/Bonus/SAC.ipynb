{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNPC6xgs0FjHpA/4UXj2IkW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install gymnasium\n","!pip install swig\n","!pip install gymnasium[box2d]\n"],"metadata":{"id":"PtSYazKjWIaZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"BsfaYY-jWlnj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0FBdyoKAVoiS"},"outputs":[],"source":["# Soft Actor-Critic\n","\n","# Check out this link for the complete model explanation: https://spinningup.openai.com/en/latest/algorithms/sac.html\n","\n","# Importing the libraries\n","\n","import gymnasium as gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import random\n","from collections import deque\n","\n","# Setting the hyperparameters\n","\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.shape[0]\n","hidden_dim = 256\n","actor_lr = 3e-4\n","critic_lr = 3e-4\n","alpha_lr = 3e-4\n","gamma = 0.99\n","tau = 0.005\n","buffer_size = 1e6\n","batch_size = 128\n","alpha = 0.2  # Entropy coefficient\n","\n","# Implementing the Replay Buffer\n","\n","class ReplayBuffer:\n","\n","    def __init__(self, capacity):\n","        self.buffer = deque(maxlen=int(capacity))\n","\n","    def push(self, state, action, reward, next_state, done):\n","        self.buffer.append((state, action, reward, next_state, done))\n","\n","    def sample(self, batch_size):\n","        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n","        return np.array(state), np.array(action), np.array(reward, dtype=np.float32), np.array(next_state), np.array(done, dtype=np.float32)\n","\n","    def __len__(self):\n","        return len(self.buffer)\n","\n","# Building the Actor Network\n","\n","class Actor(nn.Module):\n","\n","    def __init__(self):\n","        super(Actor, self).__init__()\n","        self.fc1 = nn.Linear(state_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.mean = nn.Linear(hidden_dim, action_dim)\n","        self.log_std = nn.Linear(hidden_dim, action_dim)\n","\n","    def forward(self, state):\n","        x = torch.relu(self.fc1(state))\n","        x = torch.relu(self.fc2(x))\n","        mean = self.mean(x)\n","        log_std = self.log_std(x)\n","        log_std = torch.clamp(log_std, min=-20, max=2)\n","        return mean, log_std\n","\n","    def sample(self, state):\n","        mean, log_std = self.forward(state)\n","        std = log_std.exp()\n","        normal = torch.distributions.Normal(mean, std)\n","        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n","        action = torch.tanh(x_t)\n","        return action\n","\n","# Building the Critic Network\n","\n","class Critic(nn.Module):\n","\n","    def __init__(self):\n","        super(Critic, self).__init__()\n","        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, state, action):\n","        x = torch.cat([state, action], dim=1)\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","# Building the SAC Agent\n","\n","class SACAgent:\n","\n","    def __init__(self):\n","        self.actor = Actor()\n","        self.critic_1 = Critic()\n","        self.critic_2 = Critic()\n","        self.target_critic_1 = Critic()  # ingej 2 critic shaagaad, min utgaar ni update hiigeed yvna gesen ug. tegnseer more stable ,overestimation risk bas bagasna gesen ug.\n","        self.target_critic_2 = Critic()\n","        self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n","        self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n","        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n","        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=critic_lr)\n","        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=critic_lr)\n","        self.replay_buffer = ReplayBuffer(buffer_size)\n","\n","    def select_action(self, state):\n","        state = torch.FloatTensor(state).unsqueeze(0)\n","        action = self.actor.sample(state)\n","        return action.detach().numpy()[0]\n","\n","    def update(self, batch_size, gamma=gamma, tau=tau, alpha=alpha):\n","        if len(self.replay_buffer) < batch_size:\n","            return\n","        state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n","        state = torch.FloatTensor(state)\n","        next_state = torch.FloatTensor(next_state)\n","        action = torch.FloatTensor(action)\n","        reward = torch.FloatTensor(reward).unsqueeze(1)\n","        done = torch.FloatTensor(np.float32(done)).unsqueeze(1)\n","        with torch.no_grad():\n","            next_state_action = self.actor.sample(next_state)\n","            target_q1_next = self.target_critic_1(next_state, next_state_action)\n","            target_q2_next = self.target_critic_2(next_state, next_state_action)\n","            target_q_min = torch.min(target_q1_next, target_q2_next) - alpha * torch.log(1 - next_state_action.pow(2) + 1e-6)\n","            target_q = reward + (1 - done) * gamma * target_q_min\n","        # Update of the Critic 1 network\n","        current_q1 = self.critic_1(state, action)\n","        critic_1_loss = F.mse_loss(current_q1, target_q)\n","        self.critic_1_optimizer.zero_grad()\n","        critic_1_loss.backward()\n","        self.critic_1_optimizer.step()\n","        # Update of the Critic 2 network\n","        current_q2 = self.critic_2(state, action)\n","        critic_2_loss = F.mse_loss(current_q2, target_q)\n","        self.critic_2_optimizer.zero_grad()\n","        critic_2_loss.backward()\n","        self.critic_2_optimizer.step()\n","        # Update of the Actor network\n","        entropy = torch.log(1 - self.actor.sample(state).pow(2) + 1e-6)\n","        actor_loss = (-self.critic_1(state, self.actor.sample(state)) + alpha * entropy).mean()\n","        self.actor_optimizer.zero_grad()\n","        actor_loss.backward()\n","        self.actor_optimizer.step()\n","        # Soft update of the Critic Target networks\n","        for target_param, param in zip(self.target_critic_1.parameters(), self.critic_1.parameters()):\n","            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","        for target_param, param in zip(self.target_critic_2.parameters(), self.critic_2.parameters()):\n","            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","\n","# Setting up the environment\n","\n","env = gym.make(\"CarRacing-v3\")\n","\n","# Creating the agent\n","\n","agent = SACAgent()\n","\n","# Implementing the Training Loop\n","\n","num_episodes = 100\n","for episode in range(num_episodes):\n","    state = env.reset()\n","    episode_reward = 0\n","    done = False\n","    while not done:\n","        action = agent.select_action(state)\n","        next_state, reward, done, _ = env.step(action)\n","        agent.replay_buffer.push(state, action, reward, next_state, done)\n","        agent.update(batch_size)\n","        state = next_state\n","        episode_reward += reward\n","    print(f\"Episode {episode}: Total Reward: {episode_reward}\")"]}]}