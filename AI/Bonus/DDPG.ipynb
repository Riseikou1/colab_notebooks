{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMo/Lulsj3n+j6iBHc5dY2y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# DDPG"],"metadata":{"id":"rFsiJ8WgvLhK"}},{"cell_type":"markdown","source":["Ene shit ni bol Q-value technique-iig gradient policy-toi hamtatgaj heregleed , iluu huchtei bolson shit."],"metadata":{"id":"6royPSASJqy8"}},{"cell_type":"markdown","source":["## Importing the libraries"],"metadata":{"id":"lGaV6PXwvLj9"}},{"cell_type":"code","source":["!pip install gymnasium\n","!pip install swig\n","!pip install \"gymnasium[box2d]\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"wUJqE9mZv1bM","executionInfo":{"status":"ok","timestamp":1731829040734,"user_tz":-540,"elapsed":76115,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"95640a06-0eda-4d9f-d57e-08ddacd740f9"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (1.0.0)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n","Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.2.1.post0)\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (3.1.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.1)\n","Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.1.post0)\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376104 sha256=612494890e3418a2667b0ea8cd5d2f47372b2157e177c48641ebaf0bd76d22fe\n","  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.5\n"]}]},{"cell_type":"code","source":["import gymnasium as gym\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import random\n","from collections import deque\n","import torch.optim as optim"],"metadata":{"id":"rc1KC5PgvUoR","executionInfo":{"status":"ok","timestamp":1731829047220,"user_tz":-540,"elapsed":618,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["## Setting the hyperparameters"],"metadata":{"id":"PB-tuZyQvUzt"}},{"cell_type":"code","source":["state_dim = 96*96*3\n","action_dim = 3\n","hidden_dim=256\n","batch_size = 128\n","learning_rate = 1e-4\n","gamma = 0.99\n","tau=0.005\n","buffer_size = int(1e5)\n","min_buffer_size = 1000\n","num_episodes=500\n","max_timesteps=1000"],"metadata":{"id":"FtIZ67BtvU8D","executionInfo":{"status":"ok","timestamp":1731829047747,"user_tz":-540,"elapsed":2,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## Building Actor Network"],"metadata":{"id":"qmbS17iHvVBe"}},{"cell_type":"code","source":["class Actor(nn.Module):\n","    def __init__(self):\n","        super(Actor,self).__init__()\n","        self.fc1=nn.Linear(state_dim,hidden_dim)\n","        self.fc2=nn.Linear(hidden_dim,hidden_dim)\n","        self.fc3=nn.Linear(hidden_dim,action_dim)\n","    def forward(self,state):\n","        x = F.relu(self.fc1(state))\n","        x = F.relu(self.fc2(x))\n","        action = torch.tanh(self.fc3(x)) # Tanh for bounded action\n","          # bounded action ni todorhoi range-d l baih action gesen ug.\n","        return action\n"],"metadata":{"id":"I5QH4zDcvVHv","executionInfo":{"status":"ok","timestamp":1731829047747,"user_tz":-540,"elapsed":1,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["## Building the Critic network"],"metadata":{"id":"mpUVSw_0vVNU"}},{"cell_type":"code","source":["class Critic(nn.Module):\n","    def __init__(self):\n","        super(Critic,self).__init__()\n","        self.fc1 = nn.Linear(state_dim+action_dim,hidden_dim) # state and action gesen \"pair\" yavdag tul , end nemj baina.\n","        self.fc2 = nn.Linear(hidden_dim,hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim,1)\n","    def forward(self,state,action):\n","        x = F.relu(self.fc1(torch.cat([state,action],dim=1))) # concatenate hiichij baigaa.\n","        x = F.relu(self.fc2(x))\n","        value = self.fc3(x)\n","        return value"],"metadata":{"id":"JF_BFDHCvVTO","executionInfo":{"status":"ok","timestamp":1731829048289,"user_tz":-540,"elapsed":2,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["## Implementing the Replay Buffer"],"metadata":{"id":"VrQxqlsDvVZY"}},{"cell_type":"code","source":["class ReplayBuffer():\n","    def __init__(self,capacity):\n","        self.buffer = deque(maxlen=capacity)\n","    def add(self,state,action,reward,next_state,done):\n","        self.buffer.append((state,action,reward,next_state,done))\n","    def sample(self,batch_size):\n","        return random.sample(self.buffer,batch_size)\n","    def __len__(self):\n","        return len(self.buffer)\n",""],"metadata":{"id":"Ltz-qKU2vVen","executionInfo":{"status":"ok","timestamp":1731829048289,"user_tz":-540,"elapsed":1,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["## Implementing the DDPG Algorithm"],"metadata":{"id":"Mi7lcLOOvVj9"}},{"cell_type":"code","source":["class DDPG :\n","    def __init__(self):\n","        self.actor = Actor()\n","        self.actor_target = Actor()\n","        self.actor_target.load_state_dict(self.actor.state_dict())\n","        self.actor_optimizer = optim.Adam(self.actor.parameters(),lr=learning_rate)\n","        self.critic = Critic()\n","        self.critic_target = Critic()\n","        self.critic_target.load_state_dict(self.critic.state_dict())\n","        self.critic_optimizer = optim.Adam(self.critic.parameters(),lr=learning_rate)\n","        self.replay_buffer = ReplayBuffer(buffer_size)\n","\n","    def update(self,batch_size):\n","        if len(self.replay_buffer) < min_buffer_size:\n","            return  # Exit if not enough samples\n","        samples = self.replay_buffer.sample(batch_size)\n","        states,actions,rewards,next_states,dones = map(torch.tensor,zip(*samples))\n","        # Update the Critic network:\n","        with torch.no_grad():\n","            next_actions = self.actor_target(next_states)\n","            Q_target_next = self.critic_target(next_states,next_actions) # Get Q-value for next state-action pair from target critic network\n","            Q_targets = rewards + (gamma*Q_target_next*(1-dones))   # Q_target is the sum of immediate reward and discounted future reward\n","        Q_expected = self.critic(states,actions)\n","        critic_loss = F.mse_loss(Q_expected,Q_targets)\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        # Update of the Actor network\n","        actor_loss = -self.critic(states,self.actor(states)).mean()\n","        self.actor_optimizer.zero_grad()\n","        actor_loss.backward()\n","        self.actor_optimizer.step()\n","        # Soft update of the Critic Target and the Actor target networks\n","        self.soft_update(self.critic,self.critic_target)\n","        self.soft_update(self.actor,self.actor_target)\n","    @staticmethod\n","    def soft_update(local_model,target_model):\n","        for target_param,local_param in zip(target_model.parameters(),local_model.parameters()):\n","            target_param.data.copy_(tau*local_param.data+(1.0-tau)*target_param.data)\n"],"metadata":{"id":"5Nt7aI_pvVvH","executionInfo":{"status":"ok","timestamp":1731829410956,"user_tz":-540,"elapsed":525,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["## Implementing the Training Loop"],"metadata":{"id":"2BnoHV2D-N1a"}},{"cell_type":"code","source":["def train(env,agent,num_episodes,max_timesteps):\n","    for episode in range(num_episodes):\n","        state,_ = env.reset()\n","        state = torch.tensor(state.flatten(),dtype=torch.float32) # Flatten state\n","        episode_reward = 0\n","        for t in range(max_timesteps):\n","            action = agent.actor(state).detach().numpy()\n","            next_state,reward,done,_ = env.step(action)\n","            next_state = torch.tensor(next_state.flatten(),dtype=torch.float32)\n","            agent.replay_buffer.add(state,action,reward,next_state,done)\n","            state = next_state\n","            episode_reward += reward\n","            agent.update(batch_size)\n","            if done:\n","                break\n","        print(f\"Episode {episode}: Reward = {episode_reward}\")\n"],"metadata":{"id":"t9RueJj0-N8a","executionInfo":{"status":"ok","timestamp":1731829412334,"user_tz":-540,"elapsed":1,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["## Doing other shit"],"metadata":{"id":"6YzpwScf-OCg"}},{"cell_type":"code","source":["# Setting up the environment\n","env = gym.make(\"CarRacing-v3\")\n","# Creating the agent\n","ddpg_agent = DDPG()\n","# Starting the training\n","train(env,ddpg_agent,num_episodes,max_timesteps)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":335},"id":"vQTMJp0Y-OId","executionInfo":{"status":"error","timestamp":1731829433526,"user_tz":-540,"elapsed":19743,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}},"outputId":"b60c671f-ed9b-429b-c3e3-48efa5b2988a"},"execution_count":35,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"expected Tensor as element 0 in argument 0, but got numpy.ndarray","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-35e457ccffe7>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mddpg_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDPG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Starting the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mddpg_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-34-bd63e1b7c857>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, agent, num_episodes, max_timesteps)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-33-81b61d76bd14>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Convert to PyTorch tensors using torch.stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Assuming each element in states is a 1D tensor already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Assuming rewards are scalar values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.ndarray"]}]},{"cell_type":"code","source":[],"metadata":{"id":"-KwUbmjDC8IT","executionInfo":{"status":"ok","timestamp":1731829207849,"user_tz":-540,"elapsed":4,"user":{"displayName":"temka-sama","userId":"13085788523966765980"}}},"execution_count":30,"outputs":[]}]}