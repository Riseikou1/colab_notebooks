{"cells":[{"cell_type":"markdown","metadata":{"id":"JRQ5t5bS28DG"},"source":["# Asychronous Advantage Actor-Critic for Kung-Fu"]},{"cell_type":"markdown","metadata":{"id":"qnnYUP3p28Fo"},"source":["## Part-0 Installing the required packages and importing the libraries"]},{"cell_type":"markdown","metadata":{"id":"W_jljIDe28TW"},"source":["### Installing gymnasium"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"R_58xcOi28Nx","outputId":"40793435-84a4-461d-aa7f-5519c03a4518"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting gymnasium\n","  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n","Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n","Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n","\u001b[33mWARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (3.1.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n","Collecting ale-py>=0.9 (from gymnasium[accept-rom-license,atari])\n","  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n","Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ale-py\n","Successfully installed ale-py-0.10.1\n","Requirement already satisfied: ale-py in /usr/local/lib/python3.10/dist-packages (0.10.1)\n","Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.10/dist-packages (from ale-py) (1.26.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py) (4.12.2)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  swig4.0\n","Suggested packages:\n","  swig-doc swig-examples swig4.0-examples swig4.0-doc\n","The following NEW packages will be installed:\n","  swig swig4.0\n","0 upgraded, 2 newly installed, 0 to remove and 49 not upgraded.\n","Need to get 1,116 kB of archives.\n","After this operation, 5,542 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n","Fetched 1,116 kB in 1s (2,228 kB/s)\n","Selecting previously unselected package swig4.0.\n","(Reading database ... 123623 files and directories currently installed.)\n","Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n","Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n","Selecting previously unselected package swig.\n","Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n","Unpacking swig (4.0.2-1ubuntu1) ...\n","Setting up swig4.0 (4.0.2-1ubuntu1) ...\n","Setting up swig (4.0.2-1ubuntu1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (3.1.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.1)\n","Collecting swig==4.* (from gymnasium[box2d])\n","  Downloading swig-4.2.1.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n","Downloading swig-4.2.1.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349115 sha256=110845dea13d0046c3df1895ab38c8501ed97ba6067942c72fabfe703a0ec9b0\n","  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n","Successfully built box2d-py\n","Installing collected packages: swig, box2d-py\n","Successfully installed box2d-py-2.3.5 swig-4.2.1.post0\n"]}],"source":["! pip install gymnasium\n","!pip install \"gymnasium[atari,accept-rom-license]\"\n","!pip install ale-py\n","!apt-get install -y swig\n","!pip install gymnasium[box2d]\n"]},{"cell_type":"markdown","metadata":{"id":"UyBf7xEz28b3"},"source":["### Importing the libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"J_B0B1HU28fs"},"outputs":[],"source":["import cv2\n","import math\n","import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.multiprocessing as mp\n","import torch.distributions as distributions\n","from torch.distributions import Categorical\n","import ale_py\n","import gymnasium as gym\n","from gymnasium.spaces import Box\n","from gymnasium import ObservationWrapper\n","\n","import torch.optim as optim\n","from collections import deque\n","from torch.utils.data import DataLoader,TensorDataset"]},{"cell_type":"markdown","metadata":{"id":"1hPYHfjjRXDV"},"source":["## Part-1 Building the AI"]},{"cell_type":"markdown","metadata":{"id":"PzOKPxrWRXQ_"},"source":["### Creating the architecture of the Neural Network"]},{"cell_type":"code","source":["class Network(nn.Module):\n","\n","  def __init__(self, action_size):\n","    super(Network, self).__init__()\n","    self.conv1 = torch.nn.Conv2d(in_channels = 4,  out_channels = 32, kernel_size = (3,3), stride = 2)\n","    self.conv2 = torch.nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = (3,3), stride = 2)\n","    self.conv3 = torch.nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = (3,3), stride = 2)\n","    self.flatten = torch.nn.Flatten()\n","    self.fc1  = torch.nn.Linear(512, 128)\n","    self.fc2a = torch.nn.Linear(128, action_size)\n","    self.fc2s = torch.nn.Linear(128, 1)\n","\n","  def forward(self, state):\n","    x = self.conv1(state)\n","    x = F.relu(x)\n","    x = self.conv2(x)\n","    x = F.relu(x)\n","    x = self.conv3(x)\n","    x = F.relu(x)\n","    x = self.flatten(x)\n","    x = self.fc1(x)\n","    x = F.relu(x)\n","    action_values = self.fc2a(x)\n","    state_value = self.fc2s(x)[0]\n","    return action_values, state_value"],"metadata":{"id":"a0Fjx73Dcotv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ri_QQ-TYRXlD"},"source":["## Part-2 Training the AI"]},{"cell_type":"markdown","metadata":{"id":"h9kaVOW6RXnw"},"source":["### Setting up the environment"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Os0NAisFRXth","outputId":"6f01433d-1ac7-4749-8cf7-39ca678a15bc"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:517: DeprecationWarning: \u001b[33mWARN: The environment KungFuMasterDeterministic-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.deprecation(\n"]},{"name":"stdout","output_type":"stream","text":["State shape: (4, 42, 42)\n","Number actions: 14\n","Action names: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"]}],"source":["class PreprocessAtari(ObservationWrapper):\n","\n","  def __init__(self, env, height = 42, width = 42, crop = lambda img: img, dim_order = 'pytorch', color = False, n_frames = 4):\n","    super(PreprocessAtari, self).__init__(env)\n","    self.img_size = (height, width)\n","    self.crop = crop\n","    self.dim_order = dim_order\n","    self.color = color\n","    self.frame_stack = n_frames\n","    n_channels = 3 * n_frames if color else n_frames\n","    obs_shape = {'tensorflow': (height, width, n_channels), 'pytorch': (n_channels, height, width)}[dim_order]\n","    self.observation_space = Box(0.0, 1.0, obs_shape)\n","    self.frames = np.zeros(obs_shape, dtype = np.float32)\n","\n","  def reset(self):\n","    self.frames = np.zeros_like(self.frames)\n","    obs, info = self.env.reset()\n","    self.update_buffer(obs)\n","    return self.frames, info\n","\n","  def observation(self, img):\n","    img = self.crop(img)\n","    img = cv2.resize(img, self.img_size)\n","    if not self.color:\n","      if len(img.shape) == 3 and img.shape[2] == 3:\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    img = img.astype('float32') / 255.\n","    if self.color:\n","      self.frames = np.roll(self.frames, shift = -3, axis = 0)\n","    else:\n","      self.frames = np.roll(self.frames, shift = -1, axis = 0)\n","    if self.color:\n","      self.frames[-3:] = img\n","    else:\n","      self.frames[-1] = img\n","    return self.frames\n","\n","  def update_buffer(self, obs):\n","    self.frames = self.observation(obs)\n","\n","def make_env():\n","  env = gym.make(\"KungFuMasterDeterministic-v0\", render_mode = 'rgb_array')\n","  env = PreprocessAtari(env, height = 42, width = 42, crop = lambda img: img, dim_order = 'pytorch', color = False, n_frames = 4)\n","  return env\n","\n","env = make_env()\n","\n","state_shape = env.observation_space.shape\n","number_actions = env.action_space.n\n","print(\"State shape:\", state_shape)\n","print(\"Number actions:\", number_actions)\n","print(\"Action names:\", env.env.env.env.get_action_meanings())"]},{"cell_type":"markdown","metadata":{"id":"tLxdtY_TRXzO"},"source":["### Initializing the hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"USYtIR06RX3P"},"outputs":[],"source":["learning_rate = 1e-4\n","discount_factor = 0.99\n","number_environments = 10 # 10 agenttai l gesen ug ym baina."]},{"cell_type":"markdown","source":["The policy network outputs the probabilities of each action.\n","\n","\n","The critic outputs a single value for the current state, not per action."],"metadata":{"id":"9b-04kMIcrft"}},{"cell_type":"markdown","metadata":{"id":"X02R2EkVRX6n"},"source":["### Implementing the A3C class"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Hhx_lT4BRX9-"},"outputs":[],"source":["class Agent():\n","    def __init__(self,action_size):\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        self.action_size = action_size\n","        self.network = Network(action_size).to(self.device)\n","        self.optimizer = torch.optim.Adam(self.network.parameters(), lr = learning_rate)\n","\n","    def act(self,state):\n","        if state.ndim == 3 : # hervee uguu 4n inputtai bsn shit in 1dimensionoo aldaad 3 bolchuul,\n","            state = [state]  # ene ni dimension nemeed ugchij baigaan bn . Aimr sonin shaazgaaz ahaha.\n","        state = torch.tensor(state,dtype=torch.float32,device = self.device) # tegeed teriigee tensor bolgoj convert hiigeed.\n","        action_values,_ = self.network(state)  # yurn ni l nn.Module-iin forward func- ni automatoor ajillaad yvchdag yum baina. ene shit bol forward- func--iin l utgig avmaar baigaa shit.\n","        policy = F.softmax(action_values,dim=-1) # -1 gesneeree softmax ni applied across the last dimension(action gesen ug manaid bol)\n","         # tegeed bas softmax function ni q_value-nuudiig probability distribution bolgoj uurchilj baigaa.\n","        return np.array([np.random.choice(len(p),p = p) for p in policy.detach().cpu().numpy()])\n","         # Detaches the tensor from the computation graph, samples an action based on the probabilities stored in policy.\n","          # tegheer bid nart heden state orj irsen , ter toonii size-tai array butsaana. butsaahdaa(hamgiin ih probability-tai action-uudiin index-uudiig)\n","\n","    def step(self,state,action,reward,next_state,done):\n","        batch_size = state.shape[0] # state-iin first dimension ni represents the number of state oservations in the batch\n","        state = torch.tensor(state,dtype=torch.float32,device=self.device)\n","        next_state = torch.tensor(next_state,dtype=torch.float32,device=self.device)\n","        reward = torch.tensor(reward,dtype=torch.float32,device=self.device)\n","        done = torch.tensor(done,dtype=torch.bool,device=self.device).to(dtype = torch.float32)\n","\n","        action_values,state_values = self.network(state)\n","            # action value ni gives expected return for doing certain action\n","            # state value ni gives the expected return for being in a certain state\n","\n","        _,next_state_value = self.network(next_state)\n","        target_state_value = reward+ discount_factor*next_state_value*(1-done) # target_state_value represents what the value of the current state should be, based on the reward and the expected value of the next state.\n","        advantage = target_state_value - state_values # Advantage measures how much better (or worse) the action taken was compared to the expected value of the current state.\n","\n","        probs = F.softmax(action_values,dim=-1)   # nuguu lalriin actor's loss-iin tomyogoo l shaajin.\n","        log_probs = F.log_softmax(action_values,dim=-1)  # bas critic loss.  ted nariign ni olchood l weight ntree optimize shaahgeed baigaan ug ni\n","        entropy = -torch.sum(probs*log_probs,axis=-1)   # Measures the randomness in the policy. Higher entropy means the policy is more exploratory (less deterministic).\n","        batch_idx = np.arange(batch_size)  # mnai case ni 10n agent baigaa tul 0-9 hurtelh index too.\n","        logp_actions = log_probs[batch_idx,action]  # ene bol agent bolgonii songoson action-ii prob uudiig return.\n","        actor_loss = -(logp_actions*advantage.detach()).mean() - 0.001*entropy.mean()\n","\n","        critic_loss = F.mse_loss(target_state_value.detach(),state_values)  # ene der detach hiigeed bga ni gradient-iig ni l ashiglahgui shaay gejgaan.\n","        total_loss = actor_loss + critic_loss\n","\n","        self.optimizer.zero_grad() # gradient accumulation bolohoos sergiilj baigaa(umnuh iteration-uudiin)\n","        total_loss.backward()\n","        self.optimizer.step()  # Updates network weights with the gradients.\n"]},{"cell_type":"markdown","source":["state_value represents how \"good\" the current state is in terms of future expected rewards.\n","\n","next_state_value represents how \"good\" the next state is expected to be.\n","\n","The Critic minimizes the difference between target_state_value and state_value.\n","target_state_value represents what the value of the current state should be, based on the reward and the expected value of the next state.\n","\n","The Actor optimizes actions to maximize Advantage."],"metadata":{"id":"_itdNLQPWrO5"}},{"cell_type":"markdown","source":["Actor: Optimizes action selection.\n","\n","Critic: Optimizes state value estimation.\n","\n","Entropy Bonus: Encourages exploration.\n","\n","Backpropagation: Updates both Actor and Critic together."],"metadata":{"id":"nPSTn5TOXkwb"}},{"cell_type":"markdown","metadata":{"id":"urzwUbAGRYAq"},"source":["### Initializing the A3C agent"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7IbBvYC_Rv0t"},"outputs":[],"source":["agent = Agent(number_actions)"]},{"cell_type":"markdown","metadata":{"id":"QqsamEG3Rv76"},"source":["### Evaluating our A3C agent on a certain number of episodes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2Z6gTyOARwDE"},"outputs":[],"source":["def evaluate(agent, env, n_episodes=1):\n","    episodes_rewards = []\n","    for _ in range(n_episodes):\n","        state, _ = env.reset()\n","        total_reward = 0\n","        while True:\n","            action = agent.act(state)\n","            state, reward, done, info, _ = env.step(action[0]) # action ni numpy array orj ireh tul , bas bid nar evualating the agent in a non batch mode, ingeh ystoi.\n","                # non batch gedeg ni bulgeere bish buyu, one sample at a time gesen ug. zaza yurn 2 bish [2] gej orj ireh tul l ingeh ystoi.\n","            total_reward += reward\n","            if done:\n","                break\n","        episodes_rewards.append(total_reward)\n","    return episodes_rewards\n","\n","# heden episode gej oruulj ugnu , ternii toogor ni value-nuudiin list hiij yvulj baina.\n","# actually bur ehnees ni duustal hiij2 bgad buh value-nuud ni gesen ug."]},{"cell_type":"markdown","metadata":{"id":"kmSk9erXRwJy"},"source":["### Managing multiple environments simultaneously"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vBNRnXNURwQa"},"outputs":[],"source":["# buh environment-uudiig asynchronously create , reset,step hiideg function..\n","class EnvBatch:\n","    def __init__(self,n_envs=10):\n","        self.envs = [make_env() for _ in range(n_envs)]  # 10 tusdaa env buyu 10 agent l ymdaa.\n","\n","    def reset(self):\n","        _states = []\n","        for env in self.envs :\n","            _states.append(env.reset()[0])  # ted nariigaa bugdiign ni states list ruu shaaj bn.\n","        return np.array(_states)\n","\n","    def step(self,actions):   # bid nariin hiisen step bish , env step shuu. neg action-g execute hiigeeteh l gesen ug.\n","\n","        next_states,rewards,dones,infos,_ = map(np.array,zip(*[env.step(a) for env,a in zip(self.envs,actions)]))\n","                                            # Agent bolgond hargalzah action-g ni tuunii env-tai holbood, uildel hiilguuleed shaana.tegeed garsan ur dun ni tus tusdaa hadgalagdahaar map.. ntr\n","        for i in range(len(self.envs)):\n","            if dones[i]:\n","                next_states[i] = self.envs[i].reset()[0]  # hervee tegeed agent ni env-aa duusgatsan baival reset hiigeed urgeljluuleed shaajin.\n","        return next_states,rewards,dones,infos"]},{"cell_type":"markdown","metadata":{"id":"32R1JprkRwWK"},"source":["### Training the A3C agent"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4bXpQQjBRwcI","outputId":"e0f37a67-3726-4b2c-fbc0-c89ae59c5743"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/3001 [00:00<?, ?it/s]<ipython-input-7-3bbaff05cb8c>:41: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  critic_loss = F.mse_loss(target_state_value.detach(),state_values)  # ene der detach hiigeed bga ni gradient-iig ni l ashiglahgui shaay gejgaan.\n","  0%|          | 5/3001 [00:42<5:19:48,  6.40s/it] "]},{"name":"stdout","output_type":"stream","text":["Average agent reward:  950.0\n"]},{"name":"stderr","output_type":"stream","text":[" 33%|███▎      | 1005/3001 [01:47<1:19:37,  2.39s/it]"]},{"name":"stdout","output_type":"stream","text":["Average agent reward:  460.0\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 2005/3001 [02:51<39:13,  2.36s/it]  "]},{"name":"stdout","output_type":"stream","text":["Average agent reward:  680.0\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3001/3001 [03:53<00:00, 12.84it/s]"]},{"name":"stdout","output_type":"stream","text":["Average agent reward:  560.0\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import tqdm  # just for visualising the progress bar.\n","\n","env_batch = EnvBatch(number_environments)\n","batch_states = env_batch.reset()\n","\n","with tqdm.trange(0,3001) as progress_bar:  # manai training for loop-tei adilhan hemjeetei baih yostoi.\n","    for i in progress_bar :\n","        batch_actions = agent.act(batch_states)\n","        batch_next_states,batch_rewards,batch_dones,_ = env_batch.step(batch_actions)\n","        batch_rewards*=0.01 # stablize our training\n","        agent.step(batch_states,batch_actions,batch_rewards,batch_next_states,batch_dones)\n","        batch_states = batch_next_states\n","        if i%1000 == 0:\n","            print(\"Average agent reward: \",np.mean(evaluate(agent,env,n_episodes=10)))\n"]},{"cell_type":"markdown","metadata":{"id":"-VWV7DhNRwh2"},"source":["## Part-3 Visualising the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8RhZLb3fRwon"},"outputs":[],"source":["import glob\n","import io\n","import base64\n","import imageio\n","from IPython.display import HTML, display\n","\n","def show_video_of_model(agent, env):\n","  state, _ = env.reset()\n","  done = False\n","  frames = []\n","  while not done:\n","    frame = env.render()\n","    frames.append(frame)\n","    action = agent.act(state)\n","    state, reward, done, _, _ = env.step(action[0])\n","  env.close()\n","  imageio.mimsave('video.mp4', frames, fps=30)\n","\n","show_video_of_model(agent, env)\n","\n","def show_video():\n","    mp4list = glob.glob('*.mp4')\n","    if len(mp4list) > 0:\n","        mp4 = mp4list[0]\n","        video = io.open(mp4, 'r+b').read()\n","        encoded = base64.b64encode(video)\n","        display(HTML(data='''<video alt=\"test\" autoplay\n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","    else:\n","        print(\"Could not find video\")\n","\n","show_video()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m39SA-n9KjiK"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPXRJ0uEFmpTI2MZXPvY7MS"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}